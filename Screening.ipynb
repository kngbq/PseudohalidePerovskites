{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc2a737-c52c-48f5-8585-66b8ada7b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Found 19 PV candidates\n",
      "Step 2: Identified 0 existing pseudohalides\n",
      "Step 3: Generated 511 new ABX3 candidates\n",
      "Top 5 Candidates:\n",
      "       formula type  energy_above_hull  tolerance_factor\n",
      "220  FrPd(N3)3  new                NaN          0.996003\n",
      "131  BrCf(N3)3  new                NaN          0.993445\n",
      "323  CsPd(N3)3  new                NaN          0.992793\n",
      "287  FrCr(N3)3  new                NaN          0.990223\n",
      "86   BrBk(N3)3  new                NaN          0.989949\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymatgen.core import Composition\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "import warnings\n",
    "\n",
    "# Suppress minor warnings for clean output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "class PseudohalideAnalyzer:\n",
    "    def __init__(self, mp_csv_path, shannon_csv_path):\n",
    "        # Initialize datasets with validation\n",
    "        try:\n",
    "            self.mp_df = pd.read_csv(mp_csv_path)\n",
    "            self.shannon_df = pd.read_csv(shannon_csv_path)\n",
    "            self._validate_datasets()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Data loading failed: {str(e)}\")\n",
    "\n",
    "        # Pseudohalide definition with charge information\n",
    "        self.pseudohalides = {\n",
    "            'CN': {'elements': ['C', 'N'], 'charge': -1, 'radius': 1.91},\n",
    "            'SCN': {'elements': ['S', 'C', 'N'], 'charge': -1, 'radius': 2.13},\n",
    "            'N3': {'elements': ['N'], 'charge': -1, 'radius': 1.82},\n",
    "            'OCN': {'elements': ['O', 'C', 'N'], 'charge': -1, 'radius': 2.05},\n",
    "            'SeCN': {'elements': ['Se', 'C', 'N'], 'charge': -1, 'radius': 2.20},\n",
    "            'NCS': {'elements': ['N', 'C', 'S'], 'charge': -1, 'radius': 2.13}\n",
    "        }\n",
    "\n",
    "    def _validate_datasets(self):\n",
    "        \"\"\"Ensure required columns exist in datasets\"\"\"\n",
    "        required_mp_cols = {'material_id', 'formula_pretty', 'elements', \n",
    "                           'band_gap', 'is_metal', 'energy_above_hull'}\n",
    "        required_shannon_cols = {'element', 'oxidation_state', 'r_ionic'}\n",
    "        \n",
    "        if not required_mp_cols.issubset(self.mp_df.columns):\n",
    "            missing = required_mp_cols - set(self.mp_df.columns)\n",
    "            raise ValueError(f\"MP dataset missing columns: {missing}\")\n",
    "            \n",
    "        if not required_shannon_cols.issubset(self.shannon_df.columns):\n",
    "            missing = required_shannon_cols - set(self.shannon_df.columns)\n",
    "            raise ValueError(f\"Shannon dataset missing columns: {missing}\")\n",
    "\n",
    "    def filter_pv_candidates(self, min_gap=1.0, max_gap=1.8, max_hull=0.1):\n",
    "        \"\"\"Filter materials for photovoltaic potential\"\"\"\n",
    "        return self.mp_df[\n",
    "            (~self.mp_df['is_metal']) &\n",
    "            (self.mp_df['band_gap'].between(min_gap, max_gap)) &\n",
    "            (self.mp_df['energy_above_hull'] <= max_hull)\n",
    "        ].copy()\n",
    "\n",
    "    def identify_pseudohalides(self, df):\n",
    "        \"\"\"Identify compounds containing pseudohalide groups\"\"\"\n",
    "        results = []\n",
    "        for _, row in df.iterrows():\n",
    "            comp = Composition(row['formula_pretty'])\n",
    "            # Get element symbols as strings for quick lookup\n",
    "            elements = {e.symbol for e in comp.elements}\n",
    "            \n",
    "            for p_id, p_data in self.pseudohalides.items():\n",
    "                # Create pseudohalide composition\n",
    "                pseudo_comp = Composition(\"\".join(p_data['elements']))\n",
    "                \n",
    "                # Check if all pseudohalide elements exist in compound\n",
    "                if all(e.symbol in elements for e in pseudo_comp.elements):\n",
    "                    results.append({\n",
    "                        'material_id': row['material_id'],\n",
    "                        'formula': row['formula_pretty'],\n",
    "                        'pseudohalide': p_id,\n",
    "                        'band_gap': row['band_gap'],\n",
    "                        'formation_energy': row.get('formation_energy_per_atom', np.nan),\n",
    "                        'energy_above_hull': row['energy_above_hull']\n",
    "                    })\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def get_ionic_radius(self, element, oxidation_state):\n",
    "        \"\"\"Get average ionic radius with validation\"\"\"\n",
    "        radii = self.shannon_df[\n",
    "            (self.shannon_df['element'] == element) &\n",
    "            (self.shannon_df['oxidation_state'] == oxidation_state)\n",
    "        ]['r_ionic']\n",
    "        \n",
    "        if radii.empty:\n",
    "            raise ValueError(f\"No radius found for {element}+{oxidation_state}\")\n",
    "        return radii.mean()\n",
    "\n",
    "    def generate_abx3_candidates(self, a_charges=(1, 2, 3), b_charges=(2, 3, 4)):\n",
    "        \"\"\"Generate new ABX3 candidates with charge balance validation\"\"\"\n",
    "        candidates = []\n",
    "        a_elements = self.shannon_df[\n",
    "            self.shannon_df['oxidation_state'].isin(a_charges)\n",
    "        ]['element'].unique()\n",
    "        \n",
    "        b_elements = self.shannon_df[\n",
    "            self.shannon_df['oxidation_state'].isin(b_charges)\n",
    "        ]['element'].unique()\n",
    "\n",
    "        for a in a_elements:\n",
    "            for a_charge in self.shannon_df[self.shannon_df['element'] == a]['oxidation_state'].unique():\n",
    "                for b in b_elements:\n",
    "                    for b_charge in self.shannon_df[self.shannon_df['element'] == b]['oxidation_state'].unique():\n",
    "                        if (a_charge + b_charge) != 3:  # ABX3 charge balance\n",
    "                            continue\n",
    "                            \n",
    "                        for x_id, x_data in self.pseudohalides.items():\n",
    "                            try:\n",
    "                                a_radius = self.get_ionic_radius(a, a_charge)\n",
    "                                b_radius = self.get_ionic_radius(b, b_charge)\n",
    "                                x_radius = x_data['radius']\n",
    "                                \n",
    "                                # Structural compatibility\n",
    "                                tolerance = (a_radius + x_radius) / (np.sqrt(2) * (b_radius + x_radius))\n",
    "                                octahedral = b_radius / x_radius\n",
    "                                \n",
    "                                if 0.8 < tolerance < 1.1 and 0.4 < octahedral < 0.9:\n",
    "                                    candidates.append({\n",
    "                                        'A': a,\n",
    "                                        'A_charge': a_charge,\n",
    "                                        'B': b,\n",
    "                                        'B_charge': b_charge,\n",
    "                                        'X': x_id,\n",
    "                                        'tolerance_factor': tolerance,\n",
    "                                        'octahedral_factor': octahedral,\n",
    "                                        'formula': f\"{a}{b}({x_id})3\"\n",
    "                                    })\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                                \n",
    "        return pd.DataFrame(candidates)\n",
    "\n",
    "    def full_pipeline(self):\n",
    "        \"\"\"Complete analysis pipeline\"\"\"\n",
    "        # Step 1: Filter PV candidates\n",
    "        pv_candidates = self.filter_pv_candidates()\n",
    "        print(f\"Step 1: Found {len(pv_candidates)} PV candidates\")\n",
    "        \n",
    "        # Step 2: Identify existing pseudohalides\n",
    "        existing = self.identify_pseudohalides(pv_candidates)\n",
    "        print(f\"Step 2: Identified {len(existing)} existing pseudohalides\")\n",
    "        \n",
    "        # Step 3: Generate new ABX3 candidates\n",
    "        new_candidates = self.generate_abx3_candidates()\n",
    "        print(f\"Step 3: Generated {len(new_candidates)} new ABX3 candidates\")\n",
    "        \n",
    "        # Step 4: Prepare DataFrames for merging\n",
    "        new_candidates['energy_above_hull'] = np.nan\n",
    "        existing_df = existing.assign(type='existing')\n",
    "        new_df = new_candidates.assign(\n",
    "            type='new',\n",
    "            material_id=None,\n",
    "            band_gap=np.nan\n",
    "        )\n",
    "        \n",
    "        # Step 5: Combine and filter\n",
    "        combined = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        filtered = combined[\n",
    "            (combined['tolerance_factor'].between(0.9, 1.0)) &\n",
    "            (combined['octahedral_factor'].between(0.4, 0.7)) &\n",
    "            (~combined['B'].isin(['Hg', 'Cd'])) &\n",
    "            (~combined['X'].isin(['CN', 'SCN']))\n",
    "        ]\n",
    "        \n",
    "        # Step 6: Dynamic sorting\n",
    "        sort_columns = []\n",
    "        ascending = []\n",
    "        \n",
    "        if 'energy_above_hull' in filtered.columns:\n",
    "            sort_columns.append('energy_above_hull')\n",
    "            ascending.append(True)\n",
    "            \n",
    "        sort_columns.append('tolerance_factor')\n",
    "        ascending.append(False)\n",
    "        \n",
    "        return filtered.sort_values(\n",
    "            by=sort_columns,\n",
    "            ascending=ascending\n",
    "        )\n",
    "# Initialize analyzer with validated datasets\n",
    "analyzer = PseudohalideAnalyzer(\n",
    "    mp_csv_path=\"MPDataset.csv\",\n",
    "    shannon_csv_path=\"ShannonDataset.csv\"\n",
    ")\n",
    "\n",
    "# Run complete pipeline\n",
    "results = analyzer.full_pipeline()\n",
    "\n",
    "# Show final output\n",
    "print(\"Top 5 Candidates:\")\n",
    "print(results[['formula', 'type', 'energy_above_hull', 'tolerance_factor']].head(5))\n",
    "\n",
    "# Export results\n",
    "results.to_csv(\"final_pseudohalide_candidates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85af6dc7-6134-42ad-ac85-3075cb7f4ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866de87aaf2d4bf485f74c198f22912c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/4696 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7433833ce94282b90bdea07757d723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched Perovskites:\n",
      "   formula_pretty     elements  nsites  band_gap  formation_energy_per_atom  \\\n",
      "0         AcAlO3  [Ac, Al, O]       5    4.1024                  -3.690019   \n",
      "1          AcBO3   [Ac, B, O]       5    0.8071                  -2.475390   \n",
      "2         AcCrO3  [Ac, Cr, O]       5    2.0031                  -3.138972   \n",
      "3         AcCuO3  [Ac, Cu, O]       5    0.0000                  -2.422892   \n",
      "4         AcFeO3  [Ac, Fe, O]       5    0.9888                  -2.771539   \n",
      "\n",
      "   energy_above_hull  is_stable  \n",
      "0           0.000000       True  \n",
      "1           0.792473      False  \n",
      "2           0.000000       True  \n",
      "3           0.000000       True  \n",
      "4           0.000000       True  \n",
      "Total entries: 1837\n",
      "Shannon Tail:\n",
      "      Unnamed: 0 element  oxidation_state element_oxidation_state  \\\n",
      "516         NaN      Ge                4                     NaN   \n",
      "517         NaN      Mg                2                     NaN   \n",
      "518         NaN      Ti                4                     NaN   \n",
      "519         NaN      Tl                3                     NaN   \n",
      "520         NaN      Zn                2                     NaN   \n",
      "\n",
      "    oxidation_type  r_crystal remark  r_ionic  \n",
      "516            NaN        NaN    NaN    0.530  \n",
      "517            NaN        NaN    NaN    0.720  \n",
      "518            NaN        NaN    NaN    0.605  \n",
      "519            NaN        NaN    NaN    0.885  \n",
      "520            NaN        NaN    NaN    0.740  \n",
      "Merged Dataset:\n",
      "   formula_pretty   r_A    r_B  r_X  X_A   X_B   X_X\n",
      "0         AcAlO3  1.12  0.535  1.4  1.1  1.61  3.44\n",
      "1          AcBO3  1.12  0.270  1.4  1.1  2.04  3.44\n",
      "2         AcCrO3  1.12  0.615  1.4  1.1  1.66  3.44\n",
      "3         AcCuO3  1.12  0.540  1.4  1.1  1.90  3.44\n",
      "4         AcFeO3  1.12  0.645  1.4  1.1  1.83  3.44\n",
      "5         AcGaO3  1.12  0.470  1.4  1.1  1.81  3.44\n",
      "6         AcMgO3  1.12    NaN  1.4  1.1  1.31  3.44\n",
      "7         AcMnO3  1.12  0.580  1.4  1.1  1.55  3.44\n",
      "8         AcNiO3  1.12  0.560  1.4  1.1  1.91  3.44\n",
      "9         AcPdO3  1.12    NaN  NaN  1.1  3.44  2.20\n",
      "Engineered Features:\n",
      "    formula_pretty  tolerance_factor  octahedral_factor  \\\n",
      "0          AcAlO3          0.920883           0.382143   \n",
      "1           AcBO3          1.067011           0.192857   \n",
      "2          AcCrO3          0.884322           0.439286   \n",
      "3          AcCuO3          0.918510           0.385714   \n",
      "4          AcFeO3          0.871349           0.460714   \n",
      "5          AcGaO3          0.952893           0.335714   \n",
      "7          AcMnO3          0.899954           0.414286   \n",
      "8          AcNiO3          0.909137           0.400000   \n",
      "32          AgBO3          0.910347           0.192857   \n",
      "35         AgBrO3          0.763960           0.421429   \n",
      "\n",
      "    formation_energy_per_atom  \n",
      "0                   -3.690019  \n",
      "1                   -2.475390  \n",
      "2                   -3.138972  \n",
      "3                   -2.422892  \n",
      "4                   -2.771539  \n",
      "5                   -3.063253  \n",
      "7                   -2.973630  \n",
      "8                   -2.455477  \n",
      "32                  -0.546614  \n",
      "35                  -0.285036  \n",
      "Entries after dropna: 136\n",
      "CsPbSCN3: t = 0.815, mu = 0.610\n",
      "CsSnSCN3: t = 0.970, mu = 0.354\n",
      "RbPbSCN3: t = 0.781, mu = 0.610\n",
      "RbSnSCN3: t = 0.929, mu = 0.354\n",
      "Screened Candidates (Strict):\n",
      "     formula  tolerance_factor  octahedral_factor   r_A   r_B   r_X   X_A  \\\n",
      "0  CsPbSCN3            0.8152           0.610256  1.67  1.19  1.95  0.79   \n",
      "\n",
      "    X_B  X_X  formation_energy_per_atom  \n",
      "0  2.33  2.5                        NaN  \n",
      "Screened Candidates (Relaxed):\n",
      "     formula  tolerance_factor  octahedral_factor   r_A   r_B   r_X   X_A  \\\n",
      "0  CsPbSCN3          0.815200           0.610256  1.67  1.19  1.95  0.79   \n",
      "1  CsSnSCN3          0.969593           0.353846  1.67  0.69  1.95  0.79   \n",
      "2  RbPbSCN3          0.781421           0.610256  1.52  1.19  1.95  0.82   \n",
      "3  RbSnSCN3          0.929417           0.353846  1.52  0.69  1.95  0.82   \n",
      "\n",
      "    X_B  X_X  formation_energy_per_atom  \n",
      "0  2.33  2.5                        NaN  \n",
      "1  1.96  2.5                        NaN  \n",
      "2  2.33  2.5                        NaN  \n",
      "3  1.96  2.5                        NaN  \n",
      "Model R² Score: 0.4777478296492056\n",
      "Predictions (Strict):\n",
      "     formula  predicted_band_gap\n",
      "0  CsPbSCN3                 0.0\n",
      "Predictions (Relaxed):\n",
      "     formula  predicted_band_gap\n",
      "0  CsPbSCN3                 0.0\n",
      "1  CsSnSCN3                 0.0\n",
      "2  RbPbSCN3                 0.0\n",
      "3  RbSnSCN3                 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mp_api.client import MPRester\n",
    "\n",
    "# Replace with your API key (register at materialsproject.org)\n",
    "API_KEY = \"DGBlr4BVU3ukySoP3uILlx4rd8YTb3BC\"\n",
    "\n",
    "with MPRester(API_KEY) as mpr:\n",
    "    oxide_data = mpr.materials.summary.search(\n",
    "        formula=\"ABO3\",\n",
    "        fields=[\"formula_pretty\", \"elements\", \"nsites\", \"band_gap\", \n",
    "                \"formation_energy_per_atom\", \"energy_above_hull\", \"is_stable\"]\n",
    "    )\n",
    "    halide_data = mpr.materials.summary.search(\n",
    "        formula=\"ABX3\",\n",
    "        chemsys=[\"Cs-Pb-I\", \"Cs-Sn-I\", \"Rb-Pb-I\", \"Rb-Sn-I\", \n",
    "                 \"Cs-Pb-Br\", \"Cs-Sn-Br\", \"Rb-Pb-Br\", \"Rb-Sn-Br\",\n",
    "                 \"Cs-Pb-Cl\", \"Cs-Sn-Cl\", \"Rb-Pb-Cl\", \"Rb-Sn-Cl\"],\n",
    "        fields=[\"formula_pretty\", \"elements\", \"nsites\", \"band_gap\", \n",
    "                \"formation_energy_per_atom\", \"energy_above_hull\", \"is_stable\"]\n",
    "    )\n",
    "    perovskite_data = oxide_data + halide_data\n",
    "\n",
    "mp_df = pd.DataFrame([{\n",
    "    \"formula_pretty\": entry.formula_pretty,\n",
    "    \"elements\": [str(e) for e in entry.elements],\n",
    "    \"nsites\": entry.nsites,\n",
    "    \"band_gap\": entry.band_gap if entry.band_gap is not None else np.nan,\n",
    "    \"formation_energy_per_atom\": entry.formation_energy_per_atom if entry.formation_energy_per_atom is not None else np.nan,\n",
    "    \"energy_above_hull\": entry.energy_above_hull if entry.energy_above_hull is not None else np.nan,\n",
    "    \"is_stable\": entry.is_stable\n",
    "} for entry in perovskite_data])\n",
    "mp_df = mp_df[mp_df['nsites'] == 5]\n",
    "print(\"Fetched Perovskites:\\n\", mp_df.head())\n",
    "print(f\"Total entries: {len(mp_df)}\")\n",
    "\n",
    "# Load and expand Shannon Dataset\n",
    "shannon_df = pd.read_csv(\"ShannonDataset.csv\")\n",
    "pseudohalides = pd.DataFrame({\n",
    "    'element': ['SCN', 'CN', 'N3'],\n",
    "    'oxidation_state': [-1, -1, -1],\n",
    "    'r_ionic': [1.95, 1.9, 2.1]\n",
    "})\n",
    "extra_elements = pd.DataFrame({\n",
    "    'element': ['Cs', 'Rb', 'Pb', 'Sn', 'I', 'Br', 'Cl', 'O', 'Al', 'B', 'Cr', 'Cu', 'Fe', 'Ac', \n",
    "                'Ga', 'Tb', 'Ge', 'Mg', 'Ti', 'Tl', 'Zn'],\n",
    "    'oxidation_state': [+1, +1, +2, +2, -1, -1, -1, -2, +3, +3, +3, +2, +3, +3, \n",
    "                        +3, +3, +4, +2, +4, +3, +2],\n",
    "    'r_ionic': [1.67, 1.52, 1.19, 0.69, 1.33, 1.17, 0.99, 1.40, 0.535, 0.27, 0.615, 0.73, 0.645, 1.12, \n",
    "                0.47, 0.86, 0.53, 0.72, 0.605, 0.885, 0.74]\n",
    "})\n",
    "shannon_df = pd.concat([shannon_df, pseudohalides, extra_elements], ignore_index=True)\n",
    "shannon_df = shannon_df.drop_duplicates(subset=['element', 'oxidation_state'], keep='last')\n",
    "print(\"Shannon Tail:\\n\", shannon_df.tail())\n",
    "\n",
    "# Assign oxidation states\n",
    "def assign_oxidation_states(formula, elements):\n",
    "    if 'O' in formula:\n",
    "        return {'A': +3, 'B': +3, 'X': -2}  # Oxides: ABO₃\n",
    "    elif any(x in formula for x in ['I', 'Br', 'Cl']):\n",
    "        # Halides: ABX₃ (e.g., CsPbI₃)\n",
    "        a, b, x = elements\n",
    "        if a in ['Cs', 'Rb']:  # Common A-site cations\n",
    "            return {'A': +1, 'B': +2, 'X': -1}\n",
    "        elif b in ['Pb', 'Sn']:  # Common B-site cations\n",
    "            return {'A': +1, 'B': +2, 'X': -1}\n",
    "    return {'A': +1, 'B': +2, 'X': -1}  # Default for pseudohalides\n",
    "\n",
    "# Get radii with fallback\n",
    "def get_radii(element, ox_state, shannon_df):\n",
    "    match = shannon_df[(shannon_df['element'] == element) & \n",
    "                       (shannon_df['oxidation_state'] == ox_state)]\n",
    "    radius = match['r_ionic'].values[0] if not match.empty else None\n",
    "    if radius is None:\n",
    "        try:\n",
    "            el = Element(element)\n",
    "            radius = el.ionic_radius if el.ionic_radius else np.nan\n",
    "        except:\n",
    "            radius = np.nan\n",
    "    # print(f\"Element: {element}, Ox State: {ox_state}, Radius: {radius}\")\n",
    "    return radius\n",
    "\n",
    "# Get electronegativity with default\n",
    "def get_electronegativity(element):\n",
    "    try:\n",
    "        return Element(element).X if Element(element).X else 2.5\n",
    "    except:\n",
    "        return 2.5\n",
    "\n",
    "# Expand MP data\n",
    "mp_df_expanded = mp_df.copy()\n",
    "mp_df_expanded['A'] = mp_df_expanded['elements'].apply(lambda x: x[0])\n",
    "mp_df_expanded['B'] = mp_df_expanded['elements'].apply(lambda x: x[1])\n",
    "mp_df_expanded['X'] = mp_df_expanded['elements'].apply(lambda x: x[2])\n",
    "\n",
    "for idx, row in mp_df_expanded.iterrows():\n",
    "    ox_states = assign_oxidation_states(row['formula_pretty'], row['elements'])\n",
    "    mp_df_expanded.loc[idx, 'r_A'] = get_radii(row['A'], ox_states['A'], shannon_df)\n",
    "    mp_df_expanded.loc[idx, 'r_B'] = get_radii(row['B'], ox_states['B'], shannon_df)\n",
    "    mp_df_expanded.loc[idx, 'r_X'] = get_radii(row['X'], ox_states['X'], shannon_df)\n",
    "\n",
    "mp_df_expanded['X_A'] = mp_df_expanded['A'].apply(get_electronegativity)\n",
    "mp_df_expanded['X_B'] = mp_df_expanded['B'].apply(get_electronegativity)\n",
    "mp_df_expanded['X_X'] = mp_df_expanded['X'].apply(get_electronegativity)\n",
    "\n",
    "print(\"Merged Dataset:\\n\", mp_df_expanded[['formula_pretty', 'r_A', 'r_B', 'r_X', 'X_A', 'X_B', 'X_X']].head(10))\n",
    "\n",
    "# Feature engineering\n",
    "def calc_tolerance_factor(r_A, r_B, r_X):\n",
    "    return (r_A + r_X) / (np.sqrt(2) * (r_B + r_X)) if all([r_A, r_B, r_X]) else np.nan\n",
    "\n",
    "def calc_octahedral_factor(r_B, r_X):\n",
    "    return r_B / r_X if r_B and r_X else np.nan\n",
    "\n",
    "mp_df_expanded['tolerance_factor'] = mp_df_expanded.apply(\n",
    "    lambda row: calc_tolerance_factor(row['r_A'], row['r_B'], row['r_X']), axis=1)\n",
    "mp_df_expanded['octahedral_factor'] = mp_df_expanded.apply(\n",
    "    lambda row: calc_octahedral_factor(row['r_B'], row['r_X']), axis=1)\n",
    "\n",
    "# Include formation_energy_per_atom as a feature\n",
    "mp_df_expanded.dropna(subset=['r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor', 'band_gap'], inplace=True)\n",
    "print(\"Engineered Features:\\n\", mp_df_expanded[['formula_pretty', 'tolerance_factor', 'octahedral_factor', 'formation_energy_per_atom']].head(10))\n",
    "print(f\"Entries after dropna: {len(mp_df_expanded)}\")\n",
    "\n",
    "# Screen pseudohalide candidates\n",
    "candidates = {'A': ['Cs', 'Rb'], 'B': ['Pb', 'Sn'], 'X': ['SCN']}\n",
    "screened_candidates = []\n",
    "for a in candidates['A']:\n",
    "    for b in candidates['B']:\n",
    "        for x in candidates['X']:\n",
    "            r_A = get_radii(a, +1, shannon_df)\n",
    "            r_B = get_radii(b, +2, shannon_df)\n",
    "            r_X = get_radii(x, -1, shannon_df)\n",
    "            if pd.isna([r_A, r_B, r_X]).any():\n",
    "                continue\n",
    "            t = calc_tolerance_factor(r_A, r_B, r_X)\n",
    "            mu = calc_octahedral_factor(r_B, r_X)\n",
    "            print(f\"{a}{b}{x}3: t = {t:.3f}, mu = {mu:.3f}\")\n",
    "            if 0.8 <= t <= 1.0 and 0.4 <= mu <= 0.7:\n",
    "                screened_candidates.append({\n",
    "                    'formula': f'{a}{b}{x}3',\n",
    "                    'tolerance_factor': t,\n",
    "                    'octahedral_factor': mu,\n",
    "                    'r_A': r_A,\n",
    "                    'r_B': r_B,\n",
    "                    'r_X': r_X,\n",
    "                    'X_A': get_electronegativity(a),\n",
    "                    'X_B': get_electronegativity(b),\n",
    "                    'X_X': get_electronegativity(x) if x in ['I', 'Br', 'Cl'] else 2.5,\n",
    "                    'formation_energy_per_atom': np.nan  # Placeholder, not predicted here\n",
    "                })\n",
    "screened_df = pd.DataFrame(screened_candidates)\n",
    "print(\"Screened Candidates (Strict):\\n\", screened_df)\n",
    "\n",
    "screened_candidates_relaxed = []\n",
    "for a in candidates['A']:\n",
    "    for b in candidates['B']:\n",
    "        for x in candidates['X']:\n",
    "            r_A = get_radii(a, +1, shannon_df)\n",
    "            r_B = get_radii(b, +2, shannon_df)\n",
    "            r_X = get_radii(x, -1, shannon_df)\n",
    "            if pd.isna([r_A, r_B, r_X]).any():\n",
    "                continue\n",
    "            t = calc_tolerance_factor(r_A, r_B, r_X)\n",
    "            mu = calc_octahedral_factor(r_B, r_X)\n",
    "            if 0.75 <= t <= 1.05 and 0.35 <= mu <= 0.75:\n",
    "                screened_candidates_relaxed.append({\n",
    "                    'formula': f'{a}{b}{x}3',\n",
    "                    'tolerance_factor': t,\n",
    "                    'octahedral_factor': mu,\n",
    "                    'r_A': r_A,\n",
    "                    'r_B': r_B,\n",
    "                    'r_X': r_X,\n",
    "                    'X_A': get_electronegativity(a),\n",
    "                    'X_B': get_electronegativity(b),\n",
    "                    'X_X': get_electronegativity(x) if x in ['I', 'Br', 'Cl'] else 2.5,\n",
    "                    'formation_energy_per_atom': np.nan\n",
    "                })\n",
    "screened_df_relaxed = pd.DataFrame(screened_candidates_relaxed)\n",
    "print(\"Screened Candidates (Relaxed):\\n\", screened_df_relaxed)\n",
    "\n",
    "# Machine Learning Prediction\n",
    "features = ['r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor', 'X_A', 'X_B', 'X_X', 'formation_energy_per_atom']\n",
    "X = mp_df_expanded[features]\n",
    "y = mp_df_expanded['band_gap']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = XGBRegressor(n_estimators=300, learning_rate=0.03, max_depth=4, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model R² Score:\", model.score(X_test, y_test))\n",
    "\n",
    "for df, label in [(screened_df, \"Strict\"), (screened_df_relaxed, \"Relaxed\")]:\n",
    "    if not df.empty:\n",
    "        X_new = df[features]\n",
    "        df['predicted_band_gap'] = model.predict(X_new)\n",
    "        df['predicted_band_gap'] = df['predicted_band_gap'].clip(lower=0)\n",
    "        print(f\"Predictions ({label}):\\n\", df[['formula', 'predicted_band_gap']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb63874-d7ce-4d26-a53b-0165e4b9f3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "592bf314-d518-448f-9ed2-b75bbc0dbc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 18:26:04,988 - INFO - Fetching data from Materials Project...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171f3b4f937249d9878f0685ef031cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/4696 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75adac358ccb438eb49ec8d949e28e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 18:26:08,245 - INFO - Preprocessing data...\n",
      "2025-03-13 18:26:08,250 - INFO - Remaining entries after preprocessing: 1640\n",
      "2025-03-13 18:26:08,250 - INFO - Engineering features...\n",
      "2025-03-13 18:26:08,957 - INFO - Performing chemical sanity checks...\n",
      "2025-03-13 18:26:08,958 - INFO - Training models for formation_energy_per_atom...\n",
      "2025-03-13 18:26:09,847 - INFO - Best model: xgb (R²=0.877)\n",
      "2025-03-13 18:26:09,853 - INFO - Running cross-validation...\n",
      "2025-03-13 18:26:14,099 - INFO - Generating model validation visualizations...\n",
      "2025-03-13 18:26:18,459 - INFO - \n",
      "Final Results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB   R²: 0.877  MAE: 0.206  RMSE: 0.280\n",
      "RF    R²: 0.814  MAE: 0.267  RMSE: 0.344\n",
      "GBR   R²: 0.864  MAE: 0.230  RMSE: 0.294\n",
      "NN    R²: 0.816  MAE: 0.247  RMSE: 0.342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mp_api.client import MPRester\n",
    "from pymatgen.core import Element, Composition\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    api_key: str = \"\"\n",
    "    shannon_file: str = \"ShannonDataset.csv\"\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    nn_epochs: int = 200\n",
    "    nn_patience: int = 20\n",
    "    candidate_ranges: dict = field(default_factory=lambda: {\n",
    "        'tolerance_factor': (0.75, 1.05),\n",
    "        'octahedral_factor': (0.35, 0.75)\n",
    "    })\n",
    "\n",
    "class PerovskiteNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class PerovskitePipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.shannon_df = self._prepare_shannon_data()\n",
    "        self._initialize_components()\n",
    "        self._validate_config()\n",
    "        \n",
    "    def _initialize_components(self):\n",
    "        self.candidates = {'A': ['Cs', 'Rb', 'Sr', 'Ba'], \n",
    "                          'B': ['Pb', 'Sn', 'Ge'], \n",
    "                          'X': ['SCN', 'CN', 'N3']}\n",
    "        self.models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.output_dir = Path(\"perovskite_models\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.valid_elements = {e.symbol for e in Element}\n",
    "        self.reference_df = None\n",
    "        self.X_train = self.X_test = None\n",
    "        self.y_train = self.y_test = None\n",
    "        self.test_indices = None\n",
    "\n",
    "    def _prepare_shannon_data(self):\n",
    "        shannon_df = pd.read_csv(self.config.shannon_file)\n",
    "        pseudohalides = pd.DataFrame({\n",
    "            'element': ['SCN', 'CN', 'N3'],\n",
    "            'oxidation_state': [-1, -1, -1],\n",
    "            'r_ionic': [2.13, 1.98, 2.25],\n",
    "            'electronegativity': [2.82, 3.05, 3.12],\n",
    "            'polarizability': [4.1, 3.8, 4.3]\n",
    "        })\n",
    "        return pd.concat([shannon_df, pseudohalides]).drop_duplicates(\n",
    "            subset=['element', 'oxidation_state'], keep='last')\n",
    "\n",
    "    def _validate_config(self):\n",
    "        if not 0 < self.config.test_size < 1:\n",
    "            raise ValueError(\"Test size must be between 0 and 1\")\n",
    "        if self.config.nn_patience <= 0:\n",
    "            raise ValueError(\"Patience must be positive integer\")\n",
    "\n",
    "    def fetch_data(self):\n",
    "        logging.info(\"Fetching data from Materials Project...\")\n",
    "        try:\n",
    "            with MPRester(self.config.api_key) as mpr:\n",
    "                queries = [\n",
    "                    (\"ABO3\", None, [\"Cs-Pb-I\", \"Cs-Sn-I\", \"Rb-Pb-I\"]),  # Example systems\n",
    "                    (\"ABX3\", [\"Cs-Pb-I\", \"Cs-Sn-I\", \"Rb-Pb-I\"], None)\n",
    "                ]\n",
    "                data = []\n",
    "                for formula, chemsys, fields in queries:\n",
    "                    results = mpr.materials.summary.search(\n",
    "                        formula=formula,\n",
    "                        chemsys=chemsys,\n",
    "                        fields=[\"formula_pretty\", \"elements\", \"nsites\", \n",
    "                               \"formation_energy_per_atom\", \"energy_above_hull\",\n",
    "                               \"band_gap\", \"is_stable\"]\n",
    "                    )\n",
    "                    data.extend(results)\n",
    "                return self._process_raw_data(data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data fetch failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _process_raw_data(self, raw_data):\n",
    "        processed = []\n",
    "        for entry in raw_data:\n",
    "            try:\n",
    "                processed.append({\n",
    "                    \"formula\": entry.formula_pretty,\n",
    "                    \"elements\": [str(e) for e in entry.elements],\n",
    "                    \"nsites\": entry.nsites,\n",
    "                    \"formation_energy_per_atom\": entry.formation_energy_per_atom,\n",
    "                    \"energy_above_hull\": entry.energy_above_hull,\n",
    "                    \"band_gap\": entry.band_gap,\n",
    "                    \"is_stable\": entry.is_stable\n",
    "                })\n",
    "            except AttributeError as ae:\n",
    "                logging.warning(f\"Skipping invalid entry: {str(ae)}\")\n",
    "        df = pd.DataFrame(processed)\n",
    "        return df[(df['nsites'] == 5) & \n",
    "                (df['formation_energy_per_atom'].notna())].drop_duplicates('formula')\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        logging.info(\"Preprocessing data...\")\n",
    "        # Ensure exactly 3 elements per entry\n",
    "        df = df[df['elements'].apply(lambda x: len(x) == 3)].copy()\n",
    "        df['elements'] = df['elements'].apply(\n",
    "            lambda x: [e for e in x if Element.is_valid_symbol(e)])\n",
    "        df = df.dropna(subset=['elements'])\n",
    "        logging.info(f\"Remaining entries after preprocessing: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        logging.info(\"Engineering features...\")\n",
    "        \n",
    "        # Properly extract elements from list-type column\n",
    "        df = df.assign(\n",
    "            A=df['elements'].str.get(0),  # Use .str.get() for list elements\n",
    "            B=df['elements'].str.get(1),\n",
    "            X=df['elements'].str.get(2)\n",
    "        )\n",
    "        \n",
    "        # Filter out any rows with missing elements\n",
    "        df = df.dropna(subset=['A', 'B', 'X'])\n",
    "\n",
    "        invalid = df[['A', 'B', 'X']].isna().any(axis=1)\n",
    "        if invalid.any():\n",
    "            logging.warning(f\"Dropping {invalid.sum()} rows with invalid elements\")\n",
    "            df = df[~invalid].copy()\n",
    "            \n",
    "        # Oxidation states with composition-based guessing\n",
    "        df['ox_states'] = df.apply(lambda row: self._get_oxidation_states(row), axis=1)\n",
    "        \n",
    "        # Ionic radii\n",
    "        df[['r_A', 'r_B', 'r_X']] = df.apply(\n",
    "            lambda row: self._get_ionic_radii(row), axis=1, result_type='expand')\n",
    "        \n",
    "        # Electronic properties\n",
    "        electronegativity = lambda el: Element(el).X if el in self.valid_elements else np.nan\n",
    "        for el, col in [('A', 'X_A'), ('B', 'X_B'), ('X', 'X_X')]:\n",
    "            df[col] = df[el].apply(electronegativity)\n",
    "        \n",
    "        # Structure-property relationships\n",
    "        df['tolerance_factor'] = (df['r_A'] + df['r_X']) / (np.sqrt(2) * (df['r_B'] + df['r_X']))\n",
    "        df['octahedral_factor'] = df['r_B'] / df['r_X']\n",
    "        df['delta_X'] = abs(df['X_B'] - df['X_X'])\n",
    "        \n",
    "        # Stability indicators\n",
    "        df['goldschmidt_ok'] = df['tolerance_factor'].between(0.8, 1.1).astype(int)\n",
    "        df['octahedral_ok'] = df['octahedral_factor'].between(0.4, 0.7).astype(int)\n",
    "        \n",
    "        return df.dropna(subset=['r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor'])\n",
    "\n",
    "    def _get_oxidation_states(self, row):\n",
    "        try:\n",
    "            comp = Composition(row['formula'])\n",
    "            oxi_states = comp.oxi_state_guesses()[0]\n",
    "            # Map to A/B/X positions\n",
    "            return {\n",
    "                'A': oxi_states.get(row['A']),  # Added missing parenthesis\n",
    "                'B': oxi_states.get(row['B']),\n",
    "                'X': oxi_states.get(row['X'])\n",
    "            }\n",
    "        except:\n",
    "            return {'A': +1, 'B': +2, 'X': -1}\n",
    "\n",
    "    def _get_ionic_radii(self, row):\n",
    "        try:\n",
    "            return (\n",
    "                self.shannon_df[\n",
    "                    (self.shannon_df.element == row['A']) & \n",
    "                    (self.shannon_df.oxidation_state == row['ox_states']['A'])\n",
    "                ].r_ionic.values[0],\n",
    "                self.shannon_df[\n",
    "                    (self.shannon_df.element == row['B']) & \n",
    "                    (self.shannon_df.oxidation_state == row['ox_states']['B'])\n",
    "                ].r_ionic.values[0],\n",
    "                self.shannon_df[\n",
    "                    (self.shannon_df.element == row['X']) & \n",
    "                    (self.shannon_df.oxidation_state == row['ox_states']['X'])\n",
    "                ].r_ionic.values[0]\n",
    "            )\n",
    "        except (IndexError, KeyError):  # Add KeyError catch\n",
    "            return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    def train_models(self, df, target='formation_energy_per_atom'):\n",
    "        logging.info(f\"Training models for {target}...\")\n",
    "        X = pd.get_dummies(df[['A', 'B', 'X']]).join(\n",
    "            df[['r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor', \n",
    "               'X_A', 'X_B', 'X_X', 'energy_above_hull', 'delta_X']])\n",
    "        y = df[target]\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.config.test_size, \n",
    "            random_state=self.config.random_state)\n",
    "        self.test_indices = X_test.index\n",
    "        \n",
    "        # Imputation and scaling\n",
    "        X_train = self.imputer.fit_transform(X_train)\n",
    "        X_test = self.imputer.transform(X_test)\n",
    "        self.X_train = self.scaler.fit_transform(X_train)\n",
    "        self.X_test = self.scaler.transform(X_test)\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "        \n",
    "        nn_model, train_losses, test_losses = self._train_neural_network()\n",
    "    \n",
    "        # Store all models with consistent interface\n",
    "        self.models = {\n",
    "            'xgb': XGBRegressor().fit(self.X_train, self.y_train),\n",
    "            'rf': RandomForestRegressor().fit(self.X_train, self.y_train),\n",
    "            'gbr': GradientBoostingRegressor().fit(self.X_train, self.y_train),\n",
    "            'nn': {\n",
    "                'model': nn_model,  # The actual PyTorch model\n",
    "                'train_losses': train_losses,\n",
    "                'test_losses': test_losses\n",
    "            }\n",
    "        }\n",
    "        # Model evaluation\n",
    "        results = self._evaluate_models()\n",
    "        self._save_best_model(results)\n",
    "        return results\n",
    "\n",
    "    def _plot_learning_curve(self):\n",
    "        \"\"\"Neural network training progress\"\"\"\n",
    "        if 'nn' not in self.models:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.models['nn']['train_losses'], label='Train Loss')\n",
    "        plt.plot(self.models['nn']['test_losses'], label='Validation Loss')\n",
    "        \n",
    "        plt.title(\"Neural Network Learning Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.savefig(self.output_dir/'nn_learning_curve.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    def _train_single_fold(self, X_train, y_train):\n",
    "        \"\"\"Train neural network on a single CV fold\"\"\"\n",
    "        model = PerovskiteNN(X_train.shape[1])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X_train)\n",
    "        y_tensor = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(self.config.nn_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Early stopping check\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_loss = criterion(model(X_tensor), y_tensor)\n",
    "                model.train()\n",
    "                \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= self.config.nn_patience:\n",
    "                break\n",
    "                \n",
    "        return model\n",
    "\n",
    "    def _train_neural_network(self):\n",
    "        model = PerovskiteNN(self.X_train.shape[1])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        train_tensor = torch.FloatTensor(self.X_train)\n",
    "        train_target = torch.FloatTensor(self.y_train.values).view(-1, 1)\n",
    "        test_tensor = torch.FloatTensor(self.X_test)\n",
    "        test_target = torch.FloatTensor(self.y_test.values).view(-1, 1)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        train_losses, test_losses = [], []\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(self.config.nn_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_tensor)\n",
    "            loss = criterion(outputs, train_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_loss = criterion(model(test_tensor), test_target).item()\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), self.output_dir/'nn_best.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= self.config.nn_patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        model.load_state_dict(torch.load(self.output_dir/'nn_best.pth'))\n",
    "        return model, train_losses, test_losses\n",
    "\n",
    "\n",
    "    def _evaluate_models(self):\n",
    "        \"\"\"Evaluate all models on test set\"\"\"\n",
    "        metrics = {}\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'nn':\n",
    "                # Access the actual model from the dictionary\n",
    "                with torch.no_grad():\n",
    "                    preds = model['model'](torch.FloatTensor(self.X_test)).numpy().flatten()\n",
    "            else:\n",
    "                preds = model.predict(self.X_test)\n",
    "                \n",
    "            metrics[name] = {\n",
    "                'r2': r2_score(self.y_test, preds),\n",
    "                'mae': mean_absolute_error(self.y_test, preds),\n",
    "                'rmse': np.sqrt(mean_squared_error(self.y_test, preds))\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    def _save_best_model(self, results):\n",
    "        best_model = max(results.items(), key=lambda x: x[1]['r2'])\n",
    "        logging.info(f\"Best model: {best_model[0]} (R²={best_model[1]['r2']:.3f})\")\n",
    "        joblib.dump(self.models[best_model[0]], self.output_dir/f\"{best_model[0]}_model.pkl\")\n",
    "        joblib.dump(self.scaler, self.output_dir/'scaler.pkl')\n",
    "        joblib.dump(self.imputer, self.output_dir/'imputer.pkl')\n",
    "\n",
    "    def validate_models(self, cv=5):\n",
    "        logging.info(\"Running cross-validation...\")\n",
    "        X = np.vstack([self.X_train, self.X_test])\n",
    "        y = pd.concat([self.y_train, self.y_test])\n",
    "        \n",
    "        scorers = {\n",
    "            'r2': make_scorer(r2_score),\n",
    "            'mae': make_scorer(mean_absolute_error),\n",
    "            'rmse': make_scorer(lambda y, p: np.sqrt(mean_squared_error(y, p)))\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'nn':\n",
    "                results[name] = self._cross_validate_nn(X, y, cv)\n",
    "            else:\n",
    "                cv_results = cross_validate(model, X, y, cv=cv, scoring=scorers)\n",
    "                results[name] = {\n",
    "                    'r2': cv_results['test_r2'].mean(),\n",
    "                    'mae': cv_results['test_mae'].mean(),\n",
    "                    'rmse': cv_results['test_rmse'].mean()\n",
    "                }\n",
    "        return results\n",
    "\n",
    "    def _cross_validate_nn(self, X, y, cv):\n",
    "        kf = KFold(cv)\n",
    "        scores = {'r2': [], 'mae': [], 'rmse': []}\n",
    "        \n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            model = self._train_single_fold(X_train, y_train)\n",
    "            with torch.no_grad():\n",
    "                preds = model(torch.FloatTensor(X_test)).numpy().flatten()\n",
    "            \n",
    "            scores['r2'].append(r2_score(y_test, preds))\n",
    "            scores['mae'].append(mean_absolute_error(y_test, preds))\n",
    "            scores['rmse'].append(np.sqrt(mean_squared_error(y_test, preds)))\n",
    "        \n",
    "        return {k: np.mean(v) for k, v in scores.items()}\n",
    "\n",
    "    def chemical_sanity_check(self, df):\n",
    "        logging.info(\"Performing chemical sanity checks...\")\n",
    "        valid = (\n",
    "            df['tolerance_factor'].between(0.8, 1.1) &\n",
    "            df['octahedral_factor'].between(0.4, 0.7)\n",
    "        )\n",
    "        validity_rate = valid.mean()\n",
    "        if validity_rate < 0.9:\n",
    "            warnings.warn(f\"Low validity rate: {validity_rate:.1%}\")\n",
    "        return validity_rate\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        df = self.fetch_data()\n",
    "        df = self.preprocess_data(df)\n",
    "        df = self.engineer_features(df)\n",
    "        self.chemical_sanity_check(df)\n",
    "        \n",
    "        results = self.train_models(df)\n",
    "        validation = self.validate_models()\n",
    "        \n",
    "        # This line invokes the visualizations\n",
    "        self.visualize_results(df)  # Make sure this line is present\n",
    "        \n",
    "        logging.info(\"\\nFinal Results:\")\n",
    "        for model, scores in results.items():\n",
    "            print(f\"{model.upper():<5} R²: {scores['r2']:.3f}  MAE: {scores['mae']:.3f}  RMSE: {scores['rmse']:.3f}\")\n",
    "        \n",
    "        return results, validation\n",
    "\n",
    "    ## _________VISUALIZATIONS______________\n",
    "\n",
    "    def visualize_results(self, df):\n",
    "        \"\"\"Generate comprehensive validation visualizations\"\"\"\n",
    "        logging.info(\"Generating model validation visualizations...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Parity Plots with Chemical Validity\n",
    "            self._plot_parity_with_chemicals(df)\n",
    "            \n",
    "            # 2. Learning Curves for Neural Network\n",
    "            if 'nn' in self.models:\n",
    "                self._plot_learning_curve()\n",
    "            \n",
    "            # 3. Residual Analysis\n",
    "            self._plot_residuals(df)\n",
    "            \n",
    "            # 4. Feature Importance\n",
    "            self._plot_feature_importance(df)\n",
    "            \n",
    "            # 5. Prediction Distributions\n",
    "            self._plot_prediction_distributions(df)\n",
    "            \n",
    "            # 6. Top Candidate Analysis\n",
    "            self._plot_top_candidates(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Visualization failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _plot_parity_with_chemicals(self, df):\n",
    "        # Update model prediction access\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        \n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    test_df[f'pred_{name}'] = self.models[name]['model'](\n",
    "                        torch.FloatTensor(self.X_test)\n",
    "                    ).numpy().flatten()\n",
    "            else:\n",
    "                test_df[f'pred_{name}'] = self.models[name].predict(self.X_test)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 18))\n",
    "        \n",
    "        # Plot each model's parity plot\n",
    "        for idx, (name, ax) in enumerate(zip(self.models.keys(), axes.flatten())):\n",
    "            if idx >= 4:  # Only plot first 4 models\n",
    "                break\n",
    "                \n",
    "            # Calculate metrics\n",
    "            y_true = test_df['formation_energy_per_atom']\n",
    "            y_pred = test_df[f'pred_{name}']\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            \n",
    "            # Create scatter plot with chemical validity\n",
    "            sc = ax.scatter(\n",
    "                y_true, y_pred, \n",
    "                c=test_df['tolerance_factor'],\n",
    "                cmap='viridis', \n",
    "                alpha=0.7,\n",
    "                vmin=0.7, vmax=1.1\n",
    "            )\n",
    "            \n",
    "            # Formatting\n",
    "            ax.plot([y_true.min(), y_true.max()], \n",
    "                    [y_true.min(), y_true.max()], 'r--')\n",
    "            ax.set_title(f\"{name.upper()} Parity Plot\\n(R²={r2:.3f}, MAE={mae:.3f})\")\n",
    "            ax.set_xlabel(\"True Formation Energy (eV/atom)\")\n",
    "            ax.set_ylabel(\"Predicted Formation Energy (eV/atom)\")\n",
    "            plt.colorbar(sc, ax=ax, label='Tolerance Factor')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir/'parity_plots.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_learning_curve(self):\n",
    "        \"\"\"Neural network training progress\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.models['nn']['train_losses'], label='Train Loss')\n",
    "        plt.plot(self.models['nn']['test_losses'], label='Validation Loss')\n",
    "        \n",
    "        plt.title(\"Neural Network Learning Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.savefig(self.output_dir/'nn_learning_curve.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_residuals(self, df):\n",
    "        \"\"\"Residual analysis across models\"\"\"\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for idx, name in enumerate(self.models.keys(), 1):\n",
    "            # Get predictions based on model type\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    preds = self.models[name]['model'](  # Access the actual model\n",
    "                        torch.FloatTensor(self.X_test)\n",
    "                    ).numpy().flatten()\n",
    "            else:\n",
    "                preds = self.models[name].predict(self.X_test)\n",
    "                \n",
    "            residuals = test_df['formation_energy_per_atom'] - preds\n",
    "            \n",
    "            # Plotting code remains the same\n",
    "            plt.subplot(2, 2, idx)\n",
    "            plt.scatter(preds, residuals, alpha=0.5)\n",
    "            plt.axhline(0, color='red', linestyle='--')\n",
    "            plt.title(f\"{name.upper()} Residuals\")\n",
    "            plt.xlabel(\"Predicted Values\")\n",
    "            plt.ylabel(\"Residuals\")\n",
    "            plt.grid(True)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir/'residual_analysis.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_feature_importance(self, df):\n",
    "        \"\"\"Feature importance for tree-based models\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Get feature names\n",
    "        features = pd.get_dummies(df[['A', 'B', 'X']]).columns.tolist() + [\n",
    "            'r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor',\n",
    "            'X_A', 'X_B', 'X_X', 'energy_above_hull', 'delta_X'\n",
    "        ]\n",
    "        \n",
    "        for idx, name in enumerate(self.models.keys(), 1):\n",
    "            if name == 'nn':\n",
    "                continue  # Skip NN\n",
    "                \n",
    "            model = self.models[name]\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            plt.subplot(2, 2, idx)\n",
    "            plt.title(f\"{name.upper()} Feature Importance\")\n",
    "            plt.barh(range(10), importances[indices][:10], align='center')\n",
    "            plt.yticks(range(10), [features[i] for i in indices[:10]])\n",
    "            plt.xlabel('Relative Importance')\n",
    "            plt.gca().invert_yaxis()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir/'feature_importance.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_prediction_distributions(self, df):\n",
    "        \"\"\"Distribution of predicted vs actual values\"\"\"\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    preds = self.models[name]['model'](\n",
    "                        torch.FloatTensor(self.X_test)\n",
    "                    ).numpy().flatten()\n",
    "            else:\n",
    "                preds = self.models[name].predict(self.X_test)\n",
    "                \n",
    "            plt.hist(preds, alpha=0.5, bins=30, label=name.upper())\n",
    "        \n",
    "        plt.hist(test_df['formation_energy_per_atom'], \n",
    "                bins=30, alpha=0.3, label='Actual', color='black')\n",
    "        \n",
    "        plt.title(\"Prediction Distributions vs Actual Values\")\n",
    "        plt.xlabel(\"Formation Energy (eV/atom)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.savefig(self.output_dir/'prediction_distributions.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_top_candidates(self, df):\n",
    "        \"\"\"Visualize top predicted candidates with annotations\"\"\"\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                # Access the actual model from the dictionary\n",
    "                with torch.no_grad():\n",
    "                    test_df[f'pred_{name}'] = self.models[name]['model'](  # Changed here\n",
    "                        torch.FloatTensor(self.X_test)\n",
    "                    ).numpy().flatten()\n",
    "            else:\n",
    "                test_df[f'pred_{name}'] = self.models[name].predict(self.X_test)\n",
    "        \n",
    "        # Rest of the method remains the same\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Identify top 10 most stable candidates across all models\n",
    "        top_candidates = pd.concat([\n",
    "            test_df.nsmallest(10, f'pred_{name}') for name in self.models.keys()\n",
    "        ]).drop_duplicates('formula')\n",
    "        \n",
    "        # Create parallel coordinates plot\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        pd.plotting.parallel_coordinates(\n",
    "            top_candidates[['formula', 'tolerance_factor', 'octahedral_factor'] + \n",
    "                          [f'pred_{name}' for name in self.models.keys()]],\n",
    "            'formula',\n",
    "            colormap='viridis'\n",
    "        )\n",
    "        \n",
    "        plt.title(\"Top Candidates - Multi-Model Comparison\")\n",
    "        plt.xlabel(\"Features and Model Predictions\")\n",
    "        plt.ylabel(\"Normalized Values\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir/'top_candidates.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = PipelineConfig(\n",
    "        api_key=\"0IdI9tURdQnNXU2ROBizUG4UiTye6B73\",\n",
    "        nn_epochs=150,\n",
    "        nn_patience=20\n",
    "    )\n",
    "    pipeline = PerovskitePipeline(config)\n",
    "    results, validation = pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "065fd3c8-379d-419c-bda4-dd9818f917a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b548f156a9374e7f9e3c965beb330167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Material ID: mp-1238804, Formula: Cs4PbI6, Band Gap: 3.4095 eV, Energy Above Hull: 0.0 eV/atom, Volume: 1163.7499601357163 Å³, Spacegroup: R-3c (Number: 167)\n",
      "Material ID: mp-1069538, Formula: CsPbI3, Band Gap: 1.4785 eV, Energy Above Hull: 0.025049344000002003 eV/atom, Volume: 247.0988555291859 Å³, Spacegroup: Pm-3m (Number: 221)\n",
      "Material ID: mp-1120768, Formula: CsPbI3, Band Gap: 1.6421000000000001 eV, Energy Above Hull: 0.015823317000002002 eV/atom, Volume: 1008.6977999446959 Å³, Spacegroup: Pnma (Number: 62)\n",
      "Material ID: mp-540839, Formula: CsPbI3, Band Gap: 2.5181 eV, Energy Above Hull: 0.0 eV/atom, Volume: 930.846658290349 Å³, Spacegroup: Pnma (Number: 62)\n",
      "Material ID: mp-2646981, Formula: CsPbI2Br, Band Gap: 1.4384000000000001 eV, Energy Above Hull: 0.035953406166667 eV/atom, Volume: 242.33084912816582 Å³, Spacegroup: P4/mmm (Number: 123)\n",
      "Material ID: mp-2647097, Formula: Cs2Pb(ICl)2, Band Gap: 2.9646 eV, Energy Above Hull: 0.0 eV/atom, Volume: 648.2370343292783 Å³, Spacegroup: I4/mmm (Number: 139)\n",
      "Material ID: mp-1021466, Formula: CsPPb(IF3)2, Band Gap: 1.791 eV, Energy Above Hull: 0.08003353863637 eV/atom, Volume: 341.05863915400204 Å³, Spacegroup: P4/mmm (Number: 123)\n",
      "Material ID: mp-1004530, Formula: CsBPb(IF2)2, Band Gap: 2.7223 eV, Energy Above Hull: 0.06412045749999401 eV/atom, Volume: 263.361658904732 Å³, Spacegroup: C2 (Number: 5)\n",
      "Material ID: mp-1238789, Formula: CsPb2I5, Band Gap: 1.9459 eV, Energy Above Hull: 0.037908745625001 eV/atom, Volume: 712.2071523580712 Å³, Spacegroup: I4/mcm (Number: 140)\n"
     ]
    }
   ],
   "source": [
    "from mp_api.client import MPRester\n",
    "\n",
    "with MPRester(\"0IdI9tURdQnNXU2ROBizUG4UiTye6B73\") as mpr:\n",
    "    try:\n",
    "        docs = mpr.materials.summary.search(\n",
    "            elements=[\"Cs\", \"Pb\", \"I\"],  # Materials containing Cs, Pb, I\n",
    "            fields=[\"material_id\", \"formula_pretty\", \"band_gap\", \"energy_above_hull\", \"volume\", \"symmetry\"],\n",
    "            chunk_size=10,\n",
    "            num_chunks=1\n",
    "        )\n",
    "        for doc in docs:\n",
    "            print(f\"Material ID: {doc.material_id}, \"\n",
    "                  f\"Formula: {doc.formula_pretty}, \"\n",
    "                  f\"Band Gap: {doc.band_gap} eV, \"\n",
    "                  f\"Energy Above Hull: {doc.energy_above_hull} eV/atom, \"\n",
    "                  f\"Volume: {doc.volume} Å³, \"\n",
    "                  f\"Spacegroup: {doc.symmetry.symbol} (Number: {doc.symmetry.number})\")\n",
    "    except Exception as e:\n",
    "        print(f\"🚨 Query failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4205360f-22a8-4884-8633-54038e230421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 19:38:17,253 - INFO - Loaded Shannon dataset from ./ShannonDataset.csv with shape: (497, 8)\n",
      "2025-03-13 19:38:17,253 - INFO - Shannon dataset columns: ['Unnamed: 0', 'element', 'oxidation_state', 'element_oxidation_state', 'oxidation_type', 'r_crystal', 'remark', 'r_ionic']\n",
      "2025-03-13 19:38:17,255 - WARNING - Electronegativity column missing. Adding default value of 0.\n",
      "2025-03-13 19:38:17,256 - WARNING - Polarizability column missing. Adding default value of 0.\n",
      "2025-03-13 19:38:17,259 - INFO - Processed Shannon dataset shape: (209, 5)\n",
      "2025-03-13 19:38:17,260 - INFO - Processed Shannon dataset columns: ['element', 'oxidation_state', 'r_ionic', 'electronegativity', 'polarizability']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957cc7707a4b4db2a47f418b14c7aef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 19:38:17,809 - INFO - Retrieved MP dataset shape: (9, 9)\n",
      "2025-03-13 19:38:17,810 - INFO - MP dataset features: ['material_id', 'formula_pretty', 'elements', 'nsites', 'band_gap', 'energy_above_hull', 'volume', 'symmetry', 'structure']\n",
      "2025-03-13 19:38:17,821 - INFO - Preparing initial datasets...\n",
      "2025-03-13 19:38:17,822 - INFO - Shannon dataset shape: (209, 5)\n",
      "2025-03-13 19:38:17,822 - INFO - MP dataset shape: (9, 9)\n",
      "2025-03-13 19:38:17,822 - INFO - MP dataset features: ['material_id', 'formula_pretty', 'elements', 'nsites', 'band_gap', 'energy_above_hull', 'volume', 'symmetry', 'structure']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon dataset shape: (209, 5)\n",
      "MP dataset shape: (9, 9)\n",
      "MP dataset features: ['material_id', 'formula_pretty', 'elements', 'nsites', 'band_gap', 'energy_above_hull', 'volume', 'symmetry', 'structure']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pymatgen.core import Element, Lattice, Structure\n",
    "from pymatgen.io.cif import CifWriter\n",
    "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n",
    "from mp_api.client import MPRester\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "np.random.seed(42)\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    shannon_file: str = \"ShannonDataset.csv\"\n",
    "    test_size: float = 0.2\n",
    "    val_size: float = 0.15\n",
    "    random_state: int = 42\n",
    "    mp_api_key: str = \"0IdI9tURdQnNXU2ROBizUG4UiTye6B73\"\n",
    "\n",
    "class PerovskitePipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._initialize_components()\n",
    "        self.shannon_df = self._prepare_shannon_data()\n",
    "        self.reference_df = self._prepare_mp_data()  # Use real MP data\n",
    "        self._validate_config()\n",
    "\n",
    "    def _initialize_components(self):\n",
    "        self.candidates = {'A': ['Cs', 'Rb', 'Sr', 'Ba'], \n",
    "                          'B': ['Pb', 'Sn', 'Ge'], \n",
    "                          'X': ['I', 'SCN', 'CN', 'N3']}\n",
    "        self.valid_elements = {e.symbol for e in Element}\n",
    "        self.structures_dir = \"structures/\"\n",
    "        os.makedirs(self.structures_dir, exist_ok=True)\n",
    "\n",
    "    def _validate_config(self):\n",
    "        if not 0 < self.config.test_size < 1 or not 0 < self.config.val_size < 1:\n",
    "            raise ValueError(\"Test and validation sizes must be between 0 and 1\")\n",
    "\n",
    "    def _prepare_shannon_data(self):\n",
    "        \"\"\"Load and prepare the Shannon ionic radii dataset.\"\"\"\n",
    "        try:\n",
    "            shannon_df = pd.read_csv(self.config.shannon_file)\n",
    "            logging.info(f\"Loaded Shannon dataset from {self.config.shannon_file} with shape: {shannon_df.shape}\")\n",
    "            logging.info(f\"Shannon dataset columns: {shannon_df.columns.tolist()}\")\n",
    "\n",
    "            # Define required and expected columns\n",
    "            required_columns = ['element', 'oxidation_state', 'r_ionic']\n",
    "            final_columns = ['element', 'oxidation_state', 'r_ionic', 'electronegativity', 'polarizability']\n",
    "            available_columns = shannon_df.columns.tolist()\n",
    "\n",
    "            # Check for required columns\n",
    "            missing_required = [col for col in required_columns if col not in available_columns]\n",
    "            if missing_required:\n",
    "                raise ValueError(f\"Shannon dataset is missing required columns: {missing_required}\")\n",
    "\n",
    "            # Drop extra columns\n",
    "            shannon_df = shannon_df[required_columns]\n",
    "\n",
    "            # Add electronegativity and polarizability with default values if missing\n",
    "            if 'electronegativity' not in shannon_df.columns:\n",
    "                logging.warning(\"Electronegativity column missing. Adding default value of 0.\")\n",
    "                shannon_df['electronegativity'] = 0\n",
    "            if 'polarizability' not in shannon_df.columns:\n",
    "                logging.warning(\"Polarizability column missing. Adding default value of 0.\")\n",
    "                shannon_df['polarizability'] = 0\n",
    "\n",
    "            # Check for required elements and provide defaults if missing\n",
    "            required_elements = {\n",
    "                'Cs': (1, 1.67, 0.79, 0),\n",
    "                'Rb': (1, 1.52, 0.82, 0),\n",
    "                'Pb': (2, 0.84, 2.33, 0),\n",
    "                'Sn': (2, 0.69, 1.96, 0),\n",
    "                'I': (-1, 2.20, 2.66, 0)\n",
    "            }\n",
    "            missing_elements = [el for el in required_elements if el not in shannon_df['element'].values]\n",
    "            if missing_elements:\n",
    "                logging.warning(f\"Missing ionic radii for elements: {missing_elements}. Using defaults.\")\n",
    "                defaults = pd.DataFrame({\n",
    "                    'element': list(required_elements.keys()),\n",
    "                    'oxidation_state': [data[0] for data in required_elements.values()],\n",
    "                    'r_ionic': [data[1] for data in required_elements.values()],\n",
    "                    'electronegativity': [data[2] for data in required_elements.values()],\n",
    "                    'polarizability': [data[3] for data in required_elements.values()]\n",
    "                })\n",
    "                shannon_df = pd.concat([shannon_df, defaults]).drop_duplicates(subset=['element', 'oxidation_state'], keep='last')\n",
    "\n",
    "            # Ensure pseudohalides are included\n",
    "            pseudohalides = pd.DataFrame({\n",
    "                'element': ['SCN', 'CN', 'N3'],\n",
    "                'oxidation_state': [-1, -1, -1],\n",
    "                'r_ionic': [2.13, 1.98, 2.25],\n",
    "                'electronegativity': [2.82, 3.05, 3.12],\n",
    "                'polarizability': [4.1, 3.8, 4.3]\n",
    "            })\n",
    "            shannon_df = pd.concat([shannon_df, pseudohalides]).drop_duplicates(\n",
    "                subset=['element', 'oxidation_state'], keep='last')\n",
    "\n",
    "            # Ensure final DataFrame has exactly the expected columns\n",
    "            shannon_df = shannon_df[final_columns]\n",
    "            logging.info(f\"Processed Shannon dataset shape: {shannon_df.shape}\")\n",
    "            logging.info(f\"Processed Shannon dataset columns: {shannon_df.columns.tolist()}\")\n",
    "            return shannon_df\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"ShannonDataset.csv not found at {self.config.shannon_file}. Using defaults.\")\n",
    "            defaults = pd.DataFrame({\n",
    "                'element': ['Cs', 'Rb', 'Pb', 'Sn', 'I'],\n",
    "                'oxidation_state': [1, 1, 2, 2, -1],\n",
    "                'r_ionic': [1.67, 1.52, 0.84, 0.69, 2.20],\n",
    "                'electronegativity': [0.79, 0.82, 2.33, 1.96, 2.66],\n",
    "                'polarizability': [0, 0, 0, 0, 0]\n",
    "            })\n",
    "            pseudohalides = pd.DataFrame({\n",
    "                'element': ['SCN', 'CN', 'N3'],\n",
    "                'oxidation_state': [-1, -1, -1],\n",
    "                'r_ionic': [2.13, 1.98, 2.25],\n",
    "                'electronegativity': [2.82, 3.05, 3.12],\n",
    "                'polarizability': [4.1, 3.8, 4.3]\n",
    "            })\n",
    "            shannon_df = pd.concat([defaults, pseudohalides]).drop_duplicates(\n",
    "                subset=['element', 'oxidation_state'], keep='last')\n",
    "            shannon_df = shannon_df[final_columns]\n",
    "            logging.info(f\"Default Shannon dataset shape: {shannon_df.shape}\")\n",
    "            logging.info(f\"Default Shannon dataset columns: {shannon_df.columns.tolist()}\")\n",
    "            return shannon_df\n",
    "\n",
    "    def _prepare_mp_data(self):\n",
    "        \"\"\"Retrieve data from Materials Project API.\"\"\"\n",
    "        try:\n",
    "            with MPRester(self.config.mp_api_key) as mpr:\n",
    "                docs = mpr.materials.summary.search(\n",
    "                    elements=[\"Cs\", \"Pb\", \"I\"],\n",
    "                    fields=[\"material_id\", \"formula_pretty\", \"elements\", \"nsites\", \"band_gap\", \n",
    "                            \"energy_above_hull\", \"volume\", \"symmetry\", \"structure\"],\n",
    "                    chunk_size=50,\n",
    "                    num_chunks=1\n",
    "                )\n",
    "                mp_data_list = []\n",
    "                for doc in docs:\n",
    "                    mp_data_list.append({\n",
    "                        \"material_id\": doc.material_id,\n",
    "                        \"formula_pretty\": doc.formula_pretty,\n",
    "                        \"elements\": [str(elem) for elem in doc.elements],  # Convert Element objects to strings\n",
    "                        \"nsites\": doc.nsites,\n",
    "                        \"band_gap\": doc.band_gap,\n",
    "                        \"energy_above_hull\": doc.energy_above_hull,\n",
    "                        \"volume\": doc.volume,\n",
    "                        \"symmetry\": doc.symmetry,\n",
    "                        \"structure\": doc.structure\n",
    "                    })\n",
    "                if mp_data_list:\n",
    "                    mp_data = pd.DataFrame(mp_data_list)\n",
    "                    logging.info(f\"Retrieved MP dataset shape: {mp_data.shape}\")\n",
    "                    logging.info(f\"MP dataset features: {mp_data.columns.tolist()}\")\n",
    "\n",
    "                    # Save structures as CIF files\n",
    "                    for idx, row in mp_data.iterrows():\n",
    "                        if row['structure'] is not None:\n",
    "                            CifWriter(row[\"structure\"]).write_file(f\"{self.structures_dir}/{row['material_id']}.cif\")\n",
    "\n",
    "                    return mp_data\n",
    "                else:\n",
    "                    logging.warning(\"No data retrieved from MP API.\")\n",
    "                    return pd.DataFrame()  # Return empty DataFrame if no data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to retrieve MP data: {str(e)}\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Load and prepare the initial datasets, returning their shapes and features.\"\"\"\n",
    "        logging.info(\"Preparing initial datasets...\")\n",
    "        shannon_shape = self.shannon_df.shape\n",
    "        mp_shape = self.reference_df.shape\n",
    "        mp_features = self.reference_df.columns.tolist()\n",
    "        logging.info(f\"Shannon dataset shape: {shannon_shape}\")\n",
    "        logging.info(f\"MP dataset shape: {mp_shape}\")\n",
    "        logging.info(f\"MP dataset features: {mp_features}\")\n",
    "        return shannon_shape, mp_shape, mp_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the pipeline with the configuration\n",
    "    config = PipelineConfig(shannon_file=\"./ShannonDataset.csv\")\n",
    "    pipeline = PerovskitePipeline(config)\n",
    "    \n",
    "    # Prepare the data and get shapes and features\n",
    "    shannon_shape, mp_shape, mp_features = pipeline.prepare_data()\n",
    "    print(f\"Shannon dataset shape: {shannon_shape}\")\n",
    "    print(f\"MP dataset shape: {mp_shape}\")\n",
    "    print(f\"MP dataset features: {mp_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feef279c-ff11-44bf-86e2-a20ea837f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing Data sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46145ab6-c7dc-4f18-b1c9-9e2a813e9a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426392efbf19408c82e7fa4df951f188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/4700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e69036a12234356995215a58a7825ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mp_api.client import MPRester  # Use mp_api for both MP and OQMD\n",
    "import joblib\n",
    "from dataclasses import dataclass, field\n",
    "from chemdataextractor import Document\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pymatgen.core.composition import Composition, Element\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_validate  # Corrected typo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    api_key: str = \"0IdI9tURdQnNXU2ROBizUG4UiTye6B73\"\n",
    "    shannon_file: str = \"ShannonDataset.csv\"\n",
    "    test_size: float = 0.2\n",
    "    val_size: float = 0.15\n",
    "    random_state: int = 42\n",
    "    nn_epochs: int = 150\n",
    "    nn_patience: int = 20\n",
    "    candidate_ranges: dict = field(default_factory=lambda: {\n",
    "        'tolerance_factor': (0.75, 1.05),\n",
    "        'octahedral_factor': (0.35, 0.75)\n",
    "    })\n",
    "\n",
    "class PerovskiteNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class PerovskitePipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.shannon_df = self._prepare_shannon_data()\n",
    "        self._initialize_components()\n",
    "        self._validate_config()\n",
    "        \n",
    "    def _initialize_components(self):\n",
    "        self.candidates = {'A': ['Cs', 'Rb', 'Sr', 'Ba'], \n",
    "                          'B': ['Pb', 'Sn', 'Ge'], \n",
    "                          'X': ['SCN', 'CN', 'N3']}\n",
    "        self.models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.output_dir = Path(\"perovskite_models\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.valid_elements = {e.symbol for e in Element}\n",
    "        self.reference_df = None\n",
    "        self.X_train = self.X_val = self.X_test = None\n",
    "        self.y_train = self.y_val = self.y_test = None\n",
    "        self.test_indices = None\n",
    "\n",
    "    def _prepare_shannon_data(self):\n",
    "        shannon_df = pd.read_csv(self.config.shannon_file)\n",
    "        pseudohalides = pd.DataFrame({\n",
    "            'element': ['SCN', 'CN', 'N3'],\n",
    "            'oxidation_state': [-1, -1, -1],\n",
    "            'r_ionic': [2.13, 1.98, 2.25],\n",
    "            'electronegativity': [2.82, 3.05, 3.12],\n",
    "            'polarizability': [4.1, 3.8, 4.3]\n",
    "        })\n",
    "        return pd.concat([shannon_df, pseudohalides]).drop_duplicates(\n",
    "            subset=['element', 'oxidation_state'], keep='last')\n",
    "\n",
    "    def _validate_config(self):\n",
    "        if not 0 < self.config.test_size < 1 or not 0 < self.config.val_size < 1:\n",
    "            raise ValueError(\"Test and validation sizes must be between 0 and 1\")\n",
    "        if self.config.nn_patience <= 0:\n",
    "            raise ValueError(\"Patience must be positive integer\")\n",
    "\n",
    "    def fetch_icsd_data(self):\n",
    "        try:\n",
    "            icsd_data = pd.read_csv(\"icsd_data.csv\")  # Replace with actual file path\n",
    "            icsd_data['source'] = 'ICSD'\n",
    "            return icsd_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"ICSD data fetch failed: {str(e)}\")\n",
    "            return pd.DataFrame({\n",
    "                'formula': ['CsPbI3', 'CsSnI3'],\n",
    "                'elements': [['Cs', 'Pb', 'I'], ['Cs', 'Sn', 'I']],\n",
    "                'nsites': [5, 5],\n",
    "                'formation_energy_per_atom': [-0.5, -0.6],\n",
    "                'is_stable': [True, True],\n",
    "                'volume': [200.0, 210.0],\n",
    "                'source': ['ICSD', 'ICSD']\n",
    "            })\n",
    "\n",
    "    # def fetch_aflow_data(self):\n",
    "    #     try:\n",
    "    #         aflow_data = []\n",
    "    #         target_formulas = [\"CsPbI3\", \"CsSnI3\", \"RbPbI3\"]\n",
    "    #         for formula in target_formulas:\n",
    "    #             # Fetch entries containing the elements of the formula\n",
    "    #             elements = set(Composition(formula).elements)  # e.g., {Cs, Pb, I}\n",
    "    #             element_symbols = [str(el) for el in elements]\n",
    "    #             query = search(catalog=\"icsd\").filter(K.species.contains(element_symbols[0]))\n",
    "    #             for entry in query:\n",
    "    #                 # Check if the entry matches the target formula\n",
    "    #                 if getattr(entry, 'compound', '') == formula:\n",
    "    #                     aflow_data.append({\n",
    "    #                         \"formula\": getattr(entry, 'compound', formula),\n",
    "    #                         \"elements\": getattr(entry, 'species', element_symbols),\n",
    "    #                         \"nsites\": getattr(entry, 'natoms', np.nan),\n",
    "    #                         \"formation_energy_per_atom\": getattr(entry, 'enthalpy_formation_atom', np.nan),\n",
    "    #                         \"energy_above_hull\": getattr(entry, 'delta_electronic_energy', np.nan),\n",
    "    #                         \"band_gap\": getattr(entry, 'egap', np.nan),\n",
    "    #                         \"is_stable\": True if getattr(entry, 'delta_electronic_energy', 0) < 0.1 else False,\n",
    "    #                         \"volume\": getattr(entry, 'volume_cell', np.nan),\n",
    "    #                         \"source\": \"AFLOW\"\n",
    "    #                     })\n",
    "    #         return pd.DataFrame(aflow_data)\n",
    "    #     except Exception as e:\n",
    "    #         logging.error(f\"AFLOW data fetch failed: {str(e)}\")\n",
    "    #         return pd.DataFrame()\n",
    "\n",
    "    def fetch_aflow_data(self):\n",
    "        logging.warning(\"AFLOW data fetch disabled due to library incompatibility. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def fetch_literature_data(self):\n",
    "        try:\n",
    "            literature_data = []\n",
    "            with open(\"perovskite_paper.pdf\", \"rb\") as f:\n",
    "                doc = Document(f)\n",
    "                for compound in doc.cems:\n",
    "                    if \"formation_energy\" in doc.records:\n",
    "                        for record in doc.records:\n",
    "                            if hasattr(record, 'formation_energy'):\n",
    "                                literature_data.append({\n",
    "                                    \"formula\": compound.names[0],\n",
    "                                    \"elements\": compound.elements,\n",
    "                                    \"nsites\": np.nan,\n",
    "                                    \"formation_energy_per_atom\": record.formation_energy.value if record.formation_energy else np.nan,\n",
    "                                    \"is_stable\": True,\n",
    "                                    \"volume\": np.nan,\n",
    "                                    \"source\": \"Literature\"\n",
    "                                })\n",
    "            return pd.DataFrame(literature_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Literature data fetch failed: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def fetch_data(self):\n",
    "        logging.info(\"Fetching data from Materials Project, ICSD, Perovskite Database, AFLOW, and Literature...\")\n",
    "        original_level = logging.getLogger().getEffectiveLevel()\n",
    "        logging.getLogger().setLevel(logging.ERROR)\n",
    "        \n",
    "        try:\n",
    "            with MPRester(self.config.api_key) as mpr:\n",
    "                queries = [(\"ABO3\", None, [\"Cs-Pb-I\", \"Cs-Sn-I\", \"Rb-Pb-I\"]),\n",
    "                           (\"ABX3\", [\"Cs-Pb-I\", \"Cs-Sn-I\", \"Rb-Pb-I\"], None)]\n",
    "                mp_data = []\n",
    "                for formula, chemsys, fields in queries:\n",
    "                    results = mpr.materials.summary.search(\n",
    "                        formula=formula, chemsys=chemsys,\n",
    "                        fields=[\"formula_pretty\", \"elements\", \"nsites\", \n",
    "                                \"formation_energy_per_atom\", \"energy_above_hull\",\n",
    "                                \"band_gap\", \"is_stable\", \"volume\"])\n",
    "                    mp_data.extend(results)\n",
    "                logging.debug(f\"Fetched {len(mp_data)} entries from Materials Project\")\n",
    "    \n",
    "            icsd_data = self.fetch_icsd_data()\n",
    "            logging.debug(f\"Fetched {len(icsd_data)} entries from ICSD\")\n",
    "    \n",
    "            try:\n",
    "                perovskite_db_data = pd.read_csv(\"perovskite_dataset.csv\")\n",
    "                perovskite_db_data = perovskite_db_data.rename(columns={\n",
    "                    'Formula': 'formula', 'FormationEnergy': 'formation_energy_per_atom',\n",
    "                    'Elements': 'elements', 'NSites': 'nsites', 'Volume': 'volume', 'IsStable': 'is_stable'})\n",
    "                perovskite_db_data['source'] = 'PerovskiteDB'\n",
    "                perovskite_db_data['elements'] = perovskite_db_data['elements'].apply(\n",
    "                    lambda x: eval(x) if isinstance(x, str) else x)\n",
    "                logging.debug(f\"Fetched {len(perovskite_db_data)} entries from Perovskite Database\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Perovskite Database data fetch failed: {str(e)}\")\n",
    "                perovskite_db_data = pd.DataFrame()\n",
    "                logging.debug(\"Perovskite Database data set to empty DataFrame\")\n",
    "    \n",
    "            aflow_data = self.fetch_aflow_data()\n",
    "            logging.debug(f\"Fetched {len(aflow_data)} entries from AFLOW\")\n",
    "    \n",
    "            literature_data = self.fetch_literature_data()\n",
    "            logging.debug(f\"Fetched {len(literature_data)} entries from Literature\")\n",
    "\n",
    "            def standardize_dataframe(df, required_cols):\n",
    "                for col in required_cols:\n",
    "                    if col not in df.columns:\n",
    "                        df[col] = np.nan\n",
    "                return df[required_cols]\n",
    "    \n",
    "            required_cols = [\"formula\", \"elements\", \"nsites\", \"formation_energy_per_atom\", \n",
    "                            \"energy_above_hull\", \"band_gap\", \"is_stable\", \"volume\", \"source\"]\n",
    "            mp_df = standardize_dataframe(self._process_raw_data(mp_data, source=\"Materials Project\"), required_cols)\n",
    "            icsd_data = standardize_dataframe(icsd_data, required_cols)\n",
    "            perovskite_db_data = standardize_dataframe(perovskite_db_data, required_cols)\n",
    "            aflow_data = standardize_dataframe(aflow_data, required_cols)\n",
    "            literature_data = standardize_dataframe(literature_data, required_cols)\n",
    "    \n",
    "            combined_df = pd.concat([mp_df, icsd_data, perovskite_db_data, aflow_data, literature_data]).drop_duplicates('formula')\n",
    "            \n",
    "            logging.getLogger().setLevel(original_level)\n",
    "            logging.info(f\"Combined dataset size: {len(combined_df)} entries\")\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            logging.getLogger().setLevel(original_level)\n",
    "            logging.error(f\"Data fetch failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _process_raw_data(self, raw_data, source):\n",
    "        processed = []\n",
    "        for entry in raw_data:\n",
    "            try:\n",
    "                processed.append({\n",
    "                    \"formula\": entry.formula_pretty,\n",
    "                    \"elements\": [str(e) for e in entry.elements],\n",
    "                    \"nsites\": entry.nsites,\n",
    "                    \"formation_energy_per_atom\": entry.formation_energy_per_atom,\n",
    "                    \"energy_above_hull\": getattr(entry, 'energy_above_hull', np.nan),\n",
    "                    \"band_gap\": entry.band_gap,\n",
    "                    \"is_stable\": entry.is_stable,\n",
    "                    \"volume\": entry.volume,\n",
    "                    \"source\": source\n",
    "                })\n",
    "            except AttributeError as ae:\n",
    "                logging.warning(f\"Skipping invalid entry in {source}: {str(ae)}\")\n",
    "        return pd.DataFrame(processed)\n",
    "\n",
    "    def _process_oqmd_data(self, raw_data):\n",
    "        processed = []\n",
    "        for entry in raw_data:\n",
    "            try:\n",
    "                comp = entry.composition\n",
    "                formula = comp.get_reduced_formula()\n",
    "                elements = [str(el) for el in comp.elements]\n",
    "                num_atoms = int(comp.num_atoms)\n",
    "                processed.append({\n",
    "                    \"formula\": formula,\n",
    "                    \"elements\": elements,\n",
    "                    \"nsites\": num_atoms,\n",
    "                    \"formation_energy_per_atom\": entry.data.get(\"formation_energy_per_atom\", np.nan),\n",
    "                    \"energy_above_hull\": entry.data.get(\"energy_above_hull\", np.nan),\n",
    "                    \"band_gap\": np.nan,\n",
    "                    \"is_stable\": entry.data.get(\"is_stable\", False),\n",
    "                    \"volume\": entry.data.get(\"volume\", np.nan),\n",
    "                    \"source\": \"OQMD\"\n",
    "                })\n",
    "            except AttributeError as ae:\n",
    "                logging.warning(f\"Skipping invalid OQMD entry: {str(ae)}\")\n",
    "        return pd.DataFrame(processed)\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        logging.info(\"Preprocessing data...\")\n",
    "        df = df[df['nsites'] == 5].dropna(subset=['formation_energy_per_atom'])\n",
    "        df['elements'] = df['elements'].apply(lambda x: [e for e in x if Element.is_valid_symbol(e)])\n",
    "        logging.info(f\"Remaining entries after preprocessing: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        logging.info(\"Engineering features...\")\n",
    "        df = df.assign(\n",
    "            A=df['elements'].str.get(0),\n",
    "            B=df['elements'].str.get(1),\n",
    "            X=df['elements'].str.get(2)\n",
    "        ).dropna(subset=['A', 'B', 'X'])\n",
    "\n",
    "        df['ox_states'] = df.apply(self._get_oxidation_states, axis=1)\n",
    "        df[['r_A', 'r_B', 'r_X']] = df.apply(self._get_ionic_radii, axis=1, result_type='expand')\n",
    "\n",
    "        electronegativity = lambda el: Element(el).X if el in self.valid_elements else np.nan\n",
    "        df['X_A'] = df['A'].map(electronegativity)\n",
    "        df['X_B'] = df['B'].map(electronegativity)\n",
    "        df['X_X'] = df['X'].map(electronegativity)\n",
    "\n",
    "        df['delta_X'] = abs(df['X_B'] - df['X_X'])\n",
    "        df['volume_per_atom'] = df['volume'] / df['nsites']\n",
    "        df['tolerance_factor'] = (df['r_A'] + df['r_X']) / (np.sqrt(2) * (df['r_B'] + df['r_X']))\n",
    "        df['octahedral_factor'] = df['r_B'] / df['r_X']\n",
    "\n",
    "        df['goldschmidt_ok'] = df['tolerance_factor'].between(0.75, 1.1).astype(int)\n",
    "        df['octahedral_ok'] = df['octahedral_factor'].between(0.35, 0.75).astype(int)\n",
    "        df['stable'] = df['energy_above_hull'] < 0.1\n",
    "\n",
    "        return df.dropna(subset=['r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor'])\n",
    "\n",
    "    def _get_oxidation_states(self, row):\n",
    "        try:\n",
    "            comp = Composition(row['formula'])\n",
    "            oxi_states = comp.oxi_state_guesses()\n",
    "            if oxi_states:\n",
    "                return {'A': oxi_states[0].get(row['A'], +1),\n",
    "                        'B': oxi_states[0].get(row['B'], +2),\n",
    "                        'X': oxi_states[0].get(row['X'], -1)}\n",
    "            return {'A': +1, 'B': +2, 'X': -1}\n",
    "        except Exception:\n",
    "            return {'A': +1, 'B': +2, 'X': -1}\n",
    "\n",
    "    def _get_ionic_radii(self, row):\n",
    "        try:\n",
    "            radii = []\n",
    "            for site, oxi in [('A', row['ox_states']['A']), ('B', row['ox_states']['B']), ('X', row['ox_states']['X'])]:\n",
    "                radius = self.shannon_df[\n",
    "                    (self.shannon_df['element'] == row[site]) & \n",
    "                    (self.shannon_df['oxidation_state'] == oxi)\n",
    "                ]['r_ionic'].values\n",
    "                radii.append(radius[0] if radius.size else np.nan)\n",
    "            return tuple(radii)\n",
    "        except (IndexError, KeyError):\n",
    "            return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    def train_models(self, df, target='formation_energy_per_atom'):\n",
    "        logging.info(f\"Training models for {target}...\")\n",
    "        computational_df = df[df['source'].isin(['Materials Project', 'OQMD', 'AFLOW'])].dropna(subset=[target])\n",
    "        experimental_df = df[df['source'].isin(['ICSD', 'PerovskiteDB', 'Literature'])].dropna(subset=[target])\n",
    "        \n",
    "        if experimental_df.empty:\n",
    "            logging.warning(\"No experimental data available (ICSD, PerovskiteDB, Literature). Skipping experimental evaluation.\")\n",
    "        else:\n",
    "            logging.info(f\"Found {len(experimental_df)} experimental entries.\")\n",
    "        \n",
    "        # Ensure indices are reset to avoid mismatch\n",
    "        computational_df = computational_df.reset_index(drop=True)\n",
    "        X_comp = pd.get_dummies(computational_df[['A', 'B', 'X', 'source']]).join(\n",
    "            computational_df[['r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor', \n",
    "                             'X_A', 'X_B', 'X_X', 'energy_above_hull', 'delta_X', 'volume_per_atom']])\n",
    "        y_comp = computational_df[target].reset_index(drop=True)\n",
    "        \n",
    "        if not experimental_df.empty:\n",
    "            experimental_df = experimental_df.reset_index(drop=True)\n",
    "            X_exp = pd.get_dummies(experimental_df[['A', 'B', 'X', 'source']]).join(\n",
    "                experimental_df[['r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor', \n",
    "                                'X_A', 'X_B', 'X_X', 'energy_above_hull', 'delta_X', 'volume_per_atom']])\n",
    "            y_exp = experimental_df[target].reset_index(drop=True)\n",
    "        \n",
    "            missing_cols = set(X_comp.columns) - set(X_exp.columns)\n",
    "            if missing_cols:\n",
    "                X_exp = pd.concat([X_exp, pd.DataFrame(0, index=X_exp.index, columns=missing_cols)], axis=1)\n",
    "            X_exp = X_exp[X_comp.columns]\n",
    "        else:\n",
    "            X_exp = np.array([])\n",
    "            y_exp = pd.Series(dtype=float)\n",
    "        \n",
    "        if X_comp.empty:\n",
    "            raise ValueError(\"No computational data available for training.\")\n",
    "        \n",
    "        # Create binned target for stratification after preprocessing\n",
    "        y_bins = pd.qcut(y_comp, q=5, labels=False, duplicates='drop')\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config.random_state)\n",
    "        train_val_indices, test_indices = next(skf.split(X_comp, y_bins))\n",
    "        \n",
    "        X_temp = X_comp.iloc[train_val_indices].reset_index(drop=True)\n",
    "        X_test = X_comp.iloc[test_indices].reset_index(drop=True)\n",
    "        y_temp = y_comp.iloc[train_val_indices].reset_index(drop=True)\n",
    "        y_test = y_comp.iloc[test_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Reindex y_bins to match X_temp\n",
    "        y_bins_temp = y_bins[train_val_indices].reset_index(drop=True)\n",
    "        \n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(X_temp)),\n",
    "            test_size=self.config.val_size/(1-self.config.test_size),\n",
    "            stratify=y_bins_temp,\n",
    "            random_state=self.config.random_state\n",
    "        )\n",
    "        X_train = X_temp.iloc[train_indices]\n",
    "        X_val = X_temp.iloc[val_indices]\n",
    "        y_train = y_temp.iloc[train_indices]\n",
    "        y_val = y_temp.iloc[val_indices]\n",
    "        self.test_indices = test_indices\n",
    "        \n",
    "        X_train = self.imputer.fit_transform(X_train)\n",
    "        X_val = self.imputer.transform(X_val)\n",
    "        X_test = self.imputer.transform(X_test)\n",
    "        if isinstance(X_exp, pd.DataFrame) and not X_exp.empty:\n",
    "            X_exp = self.imputer.transform(X_exp)\n",
    "        elif isinstance(X_exp, np.ndarray) and X_exp.size > 0:\n",
    "            X_exp = self.imputer.transform(X_exp)\n",
    "        self.X_train = self.scaler.fit_transform(X_train)\n",
    "        self.X_val = self.scaler.transform(X_val)\n",
    "        self.X_test = self.scaler.transform(X_test)\n",
    "        self.X_exp = X_exp\n",
    "        self.y_train, self.y_val, self.y_test, self.y_exp = y_train, y_val, y_test, y_exp\n",
    "        \n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'n_estimators': [100, 200]\n",
    "        }\n",
    "        xgb_grid = GridSearchCV(XGBRegressor(), param_grid, cv=5, scoring='r2')\n",
    "        xgb_grid.fit(self.X_train, self.y_train)\n",
    "        self.models['xgb'] = xgb_grid.best_estimator_\n",
    "        \n",
    "        self.models['rf'] = RandomForestRegressor().fit(self.X_train, self.y_train)\n",
    "        self.models['gbr'] = GradientBoostingRegressor().fit(self.X_train, self.y_train)\n",
    "        nn_model, train_losses, val_losses = self._train_neural_network()\n",
    "        \n",
    "        self.models['nn'] = {\n",
    "            'model': nn_model,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        }\n",
    "        \n",
    "        results = self._evaluate_models()\n",
    "        if (isinstance(self.X_exp, pd.DataFrame) and not X_exp.empty) or \\\n",
    "           (isinstance(self.X_exp, np.ndarray) and X_exp.size > 0):\n",
    "            self._evaluate_experimental()\n",
    "        else:\n",
    "            logging.warning(\"Skipping experimental evaluation due to empty experimental dataset.\")\n",
    "        self._physical_sanity_check(df)\n",
    "        self._save_best_model(results)\n",
    "        return results\n",
    "\n",
    "    def _train_neural_network(self):\n",
    "        model = PerovskiteNN(self.X_train.shape[1])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.3)\n",
    "    \n",
    "        train_tensor = torch.FloatTensor(self.X_train)\n",
    "        train_target = torch.FloatTensor(self.y_train.values).view(-1, 1)\n",
    "        val_tensor = torch.FloatTensor(self.X_val)\n",
    "        val_target = torch.FloatTensor(self.y_val.values).view(-1, 1)\n",
    "    \n",
    "        # Create a DataLoader for mini-batching\n",
    "        train_dataset = torch.utils.data.TensorDataset(train_tensor, train_target)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        train_losses, val_losses = [], []\n",
    "    \n",
    "        model.train()\n",
    "        for epoch in range(self.config.nn_epochs):\n",
    "            epoch_train_loss = 0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_train_loss += loss.item() * batch_x.size(0)\n",
    "            epoch_train_loss /= len(train_dataset)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_loss = criterion(model(val_tensor), val_target).item()\n",
    "                val_losses.append(val_loss)\n",
    "    \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), self.output_dir/'nn_best.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "    \n",
    "            scheduler.step(val_loss)\n",
    "            if patience_counter >= 10:\n",
    "                logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "        model.load_state_dict(torch.load(self.output_dir/'nn_best.pth', weights_only=True))\n",
    "        return model, train_losses, val_losses\n",
    "\n",
    "    def _evaluate_models(self):\n",
    "        metrics = {}\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    preds = model['model'](torch.FloatTensor(self.X_test)).numpy().flatten()\n",
    "            else:\n",
    "                preds = model.predict(self.X_test)\n",
    "            metrics[name] = {\n",
    "                'r2': r2_score(self.y_test, preds),\n",
    "                'mae': mean_absolute_error(self.y_test, preds),\n",
    "                'rmse': np.sqrt(mean_squared_error(self.y_test, preds)),\n",
    "                'mape': np.mean(np.abs((self.y_test - preds) / self.y_test)) * 100\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    def _evaluate_experimental(self):\n",
    "        logging.info(\"Evaluating models on experimental data...\")\n",
    "        metrics = {}\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    preds = model['model'](torch.FloatTensor(self.X_exp)).numpy().flatten()\n",
    "            else:\n",
    "                preds = model.predict(self.X_exp)\n",
    "            metrics[name] = {\n",
    "                'r2': r2_score(self.y_exp, preds),\n",
    "                'mae': mean_absolute_error(self.y_exp, preds),\n",
    "                'rmse': np.sqrt(mean_squared_error(self.y_exp, preds)),\n",
    "                'mape': np.mean(np.abs((self.y_exp - preds) / self.y_exp)) * 100\n",
    "            }\n",
    "        logging.info(\"\\nExperimental Data Results:\")\n",
    "        for model, scores in metrics.items():\n",
    "            print(f\"{model.upper():<5} R²: {scores['r2']:.3f}  MAE: {scores['mae']:.3f}  \"\n",
    "                  f\"RMSE: {scores['rmse']:.3f}  MAPE: {scores['mape']:.1f}%\")\n",
    "\n",
    "    def _physical_sanity_check(self, df):\n",
    "        logging.info(\"Performing physical sanity checks on predictions...\")\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    test_df[f'pred_{name}'] = self.models[name]['model'](\n",
    "                        torch.FloatTensor(self.X_test)).numpy().flatten()\n",
    "            else:\n",
    "                test_df[f'pred_{name}'] = self.models[name].predict(self.X_test)\n",
    "\n",
    "            stable_df = test_df[test_df['stable']]\n",
    "            if not stable_df.empty:\n",
    "                mean_pred_energy = stable_df[f'pred_{name}'].mean()\n",
    "                if mean_pred_energy > 0:\n",
    "                    warnings.warn(f\"{name.upper()}: Stable compounds have positive mean predicted formation energy ({mean_pred_energy:.3f} eV/atom)\")\n",
    "\n",
    "            corr = test_df['tolerance_factor'].corr(test_df[f'pred_{name}'])\n",
    "            if abs(corr) < 0.1:\n",
    "                warnings.warn(f\"{name.upper()}: Low correlation between tolerance factor and predicted formation energy ({corr:.3f})\")\n",
    "\n",
    "    def _cross_validate_nn(self, X, y, cv):\n",
    "        kf = KFold(n_splits=cv, shuffle=True, random_state=self.config.random_state)\n",
    "        scores = {'r2': [], 'mae': [], 'rmse': [], 'mape': []}\n",
    "        all_train_losses, all_val_losses = [], []\n",
    "    \n",
    "        for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.15, random_state=self.config.random_state)\n",
    "    \n",
    "            model, train_losses, val_losses = self._train_neural_network(X_train, X_val, y_train, y_val)\n",
    "            all_train_losses.append(train_losses)\n",
    "            all_val_losses.append(val_losses)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                preds = model(torch.FloatTensor(X_test)).numpy().flatten()\n",
    "    \n",
    "            scores['r2'].append(r2_score(y_test, preds))\n",
    "            scores['mae'].append(mean_absolute_error(y_test, preds))\n",
    "            scores['rmse'].append(np.sqrt(mean_squared_error(y_test, preds)))\n",
    "            scores['mape'].append(np.mean(np.abs((y_test - preds) / y_test)) * 100)\n",
    "    \n",
    "        # Average the learning curves across folds\n",
    "        max_len = max(len(lst) for lst in all_train_losses)\n",
    "        avg_train_losses = np.zeros(max_len)\n",
    "        avg_val_losses = np.zeros(max_len)\n",
    "        for i in range(max_len):\n",
    "            train_vals = [lst[i] for lst in all_train_losses if i < len(lst)]\n",
    "            val_vals = [lst[i] for lst in all_val_losses if i < len(lst)]\n",
    "            avg_train_losses[i] = np.mean(train_vals)\n",
    "            avg_val_losses[i] = np.mean(val_vals)\n",
    "    \n",
    "        self.models['nn']['train_losses'] = avg_train_losses\n",
    "        self.models['nn']['val_losses'] = avg_val_losses\n",
    "        return {k: np.mean(v) for k, v in scores.items()}\n",
    "\n",
    "    \n",
    "    def _save_best_model(self, results):\n",
    "        best_model = max(results.items(), key=lambda x: x[1]['r2'])\n",
    "        logging.info(f\"Best model: {best_model[0]} (R²={best_model[1]['r2']:.3f})\")\n",
    "        joblib.dump(self.models[best_model[0]], self.output_dir/f\"{best_model[0]}_model.pkl\")\n",
    "        joblib.dump(self.scaler, self.output_dir/'scaler.pkl')\n",
    "        joblib.dump(self.imputer, self.output_dir/'imputer.pkl')\n",
    "\n",
    "    def validate_models(self, cv=5):\n",
    "        logging.info(\"Running cross-validation...\")\n",
    "        X = np.vstack([self.X_train, self.X_val, self.X_test])\n",
    "        y = pd.concat([self.y_train, self.y_val, self.y_test])\n",
    "\n",
    "        scorers = {\n",
    "            'r2': make_scorer(r2_score),\n",
    "            'mae': make_scorer(mean_absolute_error),\n",
    "            'rmse': make_scorer(lambda y, p: np.sqrt(mean_squared_error(y, p))),\n",
    "            'mape': make_scorer(lambda y, p: np.mean(np.abs((y - p) / y)) * 100)\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'nn':\n",
    "                results[name] = self._cross_validate_nn(X, y, cv)\n",
    "            else:\n",
    "                cv_results = cross_validate(model, X, y, cv=cv, scoring=scorers)\n",
    "                results[name] = {k: v.mean() for k, v in cv_results.items() if k.startswith('test_')}\n",
    "        return results\n",
    "\n",
    "    def _cross_validate_nn(self, X, y, cv):\n",
    "        kf = KFold(cv)\n",
    "        scores = {'r2': [], 'mae': [], 'rmse': [], 'mape': []}\n",
    "\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.15, random_state=self.config.random_state)\n",
    "\n",
    "            model = self._train_single_fold(X_train, X_val, y_train, y_val)\n",
    "            with torch.no_grad():\n",
    "                preds = model(torch.FloatTensor(X_test)).numpy().flatten()\n",
    "\n",
    "            scores['r2'].append(r2_score(y_test, preds))\n",
    "            scores['mae'].append(mean_absolute_error(y_test, preds))\n",
    "            scores['rmse'].append(np.sqrt(mean_squared_error(y_test, preds)))\n",
    "            scores['mape'].append(np.mean(np.abs((y_test - preds) / y_test)) * 100)\n",
    "\n",
    "        return {k: np.mean(v) for k, v in scores.items()}\n",
    "\n",
    "    def _train_single_fold(self, X_train, X_val, y_train, y_val):\n",
    "        model = PerovskiteNN(X_train.shape[1])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "        train_tensor = torch.FloatTensor(X_train)\n",
    "        train_target = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "        val_tensor = torch.FloatTensor(X_val)\n",
    "        val_target = torch.FloatTensor(y_val.values).view(-1, 1)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(self.config.nn_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_tensor)\n",
    "            loss = criterion(outputs, train_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_loss = criterion(model(val_tensor), val_target).item()\n",
    "                model.train()\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), self.output_dir/'nn_fold_best.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "            if patience_counter >= self.config.nn_patience:\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(torch.load(self.output_dir/'nn_fold_best.pth', weights_only=True))\n",
    "        return model\n",
    "\n",
    "    def chemical_sanity_check(self, df):\n",
    "        logging.info(\"Performing chemical sanity checks...\")\n",
    "        valid = (\n",
    "            df['tolerance_factor'].between(0.75, 1.1) &\n",
    "            df['octahedral_factor'].between(0.35, 0.75)\n",
    "        )\n",
    "        validity_rate = valid.mean()\n",
    "        if validity_rate < 0.8:\n",
    "            warnings.warn(f\"Low validity rate: {validity_rate:.1%}\")\n",
    "        return validity_rate\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        df = self.fetch_data()\n",
    "        df = self.preprocess_data(df)\n",
    "        df = self.engineer_features(df)\n",
    "        self.chemical_sanity_check(df)\n",
    "\n",
    "        results = self.train_models(df)\n",
    "        validation = self.validate_models()\n",
    "\n",
    "        self.visualize_results(df)\n",
    "\n",
    "        logging.info(\"\\nFinal Results (Computational Test Set):\")\n",
    "        for model, scores in results.items():\n",
    "            print(f\"{model.upper():<5} R²: {scores['r2']:.3f}  MAE: {scores['mae']:.3f}  \"\n",
    "                  f\"RMSE: {scores['rmse']:.3f}  MAPE: {scores['mape']:.1f}%\")\n",
    "        return results, validation\n",
    "\n",
    "    def visualize_results(self, df):\n",
    "        logging.info(\"Generating model validation visualizations...\")\n",
    "        try:\n",
    "            self._plot_parity_with_chemicals(df)\n",
    "            if 'nn' in self.models:\n",
    "                self._plot_learning_curve()\n",
    "            self._plot_residuals(df)\n",
    "            self._plot_feature_importance(df)\n",
    "            self._plot_prediction_distributions(df)\n",
    "            self._plot_top_candidates(df)\n",
    "            self._plot_feature_correlations(df)\n",
    "            self._plot_error_by_composition(df)\n",
    "            self._plot_formation_energy_vs_tolerance(df)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Visualization failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _plot_parity_with_chemicals(self, df):\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    test_df[f'pred_{name}'] = self.models[name]['model'](\n",
    "                        torch.FloatTensor(self.X_test)\n",
    "                    ).numpy().flatten()\n",
    "            else:\n",
    "                test_df[f'pred_{name}'] = self.models[name].predict(self.X_test)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 18))\n",
    "        for idx, (name, ax) in enumerate(zip(self.models.keys(), axes.flatten())):\n",
    "            if idx >= 4:\n",
    "                break\n",
    "            y_true = test_df['formation_energy_per_atom']\n",
    "            y_pred = test_df[f'pred_{name}']\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "            sc = ax.scatter(y_true, y_pred, c=test_df['tolerance_factor'],\n",
    "                           cmap='viridis', alpha=0.7, vmin=0.7, vmax=1.1)\n",
    "            ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "            ax.set_title(f\"{name.upper()} Parity Plot\\n(R²={r2:.3f}, MAE={mae:.3f})\")\n",
    "            ax.set_xlabel(\"True Formation Energy (eV/atom)\")\n",
    "            ax.set_ylabel(\"Predicted Formation Energy (eV/atom)\")\n",
    "            plt.colorbar(sc, ax=ax, label='Tolerance Factor')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir/'parity_plots.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_learning_curve(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.models['nn']['train_losses'], label='Train Loss')\n",
    "        plt.plot(self.models['nn']['val_losses'], label='Validation Loss')\n",
    "        plt.title(\"Neural Network Learning Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE Loss\")\n",
    "        # Use a log scale to better visualize the loss trends\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\", ls=\"--\")\n",
    "        plt.savefig(self.output_dir/'nn_learning_curve.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_residuals(self, df):\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        for idx, (name, ax) in enumerate(zip(self.models.keys(), axes.flatten())):\n",
    "            if idx >= 4:\n",
    "                break\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    preds = self.models[name]['model'](torch.FloatTensor(self.X_test)).numpy().flatten()\n",
    "            else:\n",
    "                preds = self.models[name].predict(self.X_test)\n",
    "            residuals = test_df['formation_energy_per_atom'] - preds\n",
    "            ax.scatter(preds, residuals, alpha=0.5)\n",
    "            ax.axhline(0, color='red', linestyle='--')\n",
    "            ax.set_title(f\"{name.upper()} Residuals\")\n",
    "            ax.set_xlabel(\"Predicted Values\")\n",
    "            ax.set_ylabel(\"Residuals\")\n",
    "            ax.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir/'residual_analysis.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_feature_importance(self, df):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        features = pd.get_dummies(df[['A', 'B', 'X', 'source']]).columns.tolist() + [\n",
    "            'r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor',\n",
    "            'X_A', 'X_B', 'X_X', 'energy_above_hull', 'delta_X', 'volume_per_atom']\n",
    "        for idx, name in enumerate(['xgb', 'rf', 'gbr'], 1):\n",
    "            if name in self.models and hasattr(self.models[name], 'feature_importances_'):\n",
    "                importances = self.models[name].feature_importances_\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                plt.subplot(1, 3, idx)\n",
    "                plt.title(f\"{name.upper()} Feature Importance\")\n",
    "                plt.barh(range(10), importances[indices][:10], align='center')\n",
    "                plt.yticks(range(10), [features[i] for i in indices[:10]])\n",
    "                plt.xlabel('Relative Importance')\n",
    "                plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir/'feature_importance.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_prediction_distributions(self, df):\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    preds = self.models[name]['model'](torch.FloatTensor(self.X_test)).numpy().flatten()\n",
    "            else:\n",
    "                preds = self.models[name].predict(self.X_test)\n",
    "            plt.hist(preds, alpha=0.5, bins=30, label=name.upper())\n",
    "        plt.hist(test_df['formation_energy_per_atom'], bins=30, alpha=0.3, label='Actual', color='black')\n",
    "        plt.title(\"Prediction Distributions vs Actual Values\")\n",
    "        plt.xlabel(\"Formation Energy (eV/atom)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.savefig(self.output_dir/'prediction_distributions.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_top_candidates(self, df):\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    test_df[f'pred_{name}'] = self.models[name]['model'](\n",
    "                        torch.FloatTensor(self.X_test)).numpy().flatten()\n",
    "            else:\n",
    "                test_df[f'pred_{name}'] = self.models[name].predict(self.X_test)\n",
    "\n",
    "        top_candidates = pd.concat([\n",
    "            test_df.nsmallest(10, f'pred_{name}') for name in self.models.keys()\n",
    "        ]).drop_duplicates('formula')\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        pd.plotting.parallel_coordinates(\n",
    "            top_candidates[['formula', 'tolerance_factor', 'octahedral_factor'] + \n",
    "                          [f'pred_{name}' for name in self.models.keys()]],\n",
    "            'formula',\n",
    "            colormap='viridis'\n",
    "        )\n",
    "        plt.title(\"Top Candidates - Multi-Model Comparison\")\n",
    "        plt.xlabel(\"Features and Model Predictions\")\n",
    "        plt.ylabel(\"Normalized Values\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir/'top_candidates.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_feature_correlations(self, df):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        features = df[['r_A', 'r_B', 'r_X', 'tolerance_factor', 'octahedral_factor',\n",
    "                      'X_A', 'X_B', 'X_X', 'energy_above_hull', 'delta_X', 'volume_per_atom']]\n",
    "        sns.heatmap(features.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title(\"Feature Correlations\")\n",
    "        plt.savefig(self.output_dir/'feature_correlations.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_error_by_composition(self, df):\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    preds = self.models[name]['model'](torch.FloatTensor(self.X_test)).numpy().flatten()\n",
    "            else:\n",
    "                preds = self.models[name].predict(self.X_test)\n",
    "            test_df[f'error_{name}'] = np.abs(test_df['formation_energy_per_atom'] - preds)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x='A', y=f'error_xgb', data=test_df)\n",
    "        plt.title(f\"Prediction Errors by A-site Element (XGB)\")\n",
    "        plt.savefig(self.output_dir/'error_by_A_xgb.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_formation_energy_vs_tolerance(self, df):\n",
    "        test_df = df.loc[self.test_indices].copy()\n",
    "        for name in self.models.keys():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    test_df[f'pred_{name}'] = self.models[name]['model'](\n",
    "                        torch.FloatTensor(self.X_test)).numpy().flatten()\n",
    "            else:\n",
    "                test_df[f'pred_{name}'] = self.models[name].predict(self.X_test)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        handles = {}\n",
    "        for name in self.models.keys():\n",
    "            scatter = sns.scatterplot(\n",
    "                x='tolerance_factor', \n",
    "                y=f'pred_{name}', \n",
    "                hue='stable', \n",
    "                size='energy_above_hull',\n",
    "                data=test_df, \n",
    "                alpha=0.7\n",
    "            )\n",
    "            handles[name.upper()] = scatter\n",
    "\n",
    "        plt.axvline(x=1.0, color='red', linestyle='--', label='Ideal Tolerance Factor')\n",
    "        plt.title(\"Predicted Formation Energy vs Tolerance Factor\")\n",
    "        plt.xlabel(\"Tolerance Factor\")\n",
    "        plt.ylabel(\"Predicted Formation Energy (eV/atom)\")\n",
    "\n",
    "        ax = plt.gca()\n",
    "        hue_handles, hue_labels = ax.get_legend_handles_labels()\n",
    "        custom_handles = [plt.scatter([], [], label=name.upper(), color='gray') for name in self.models.keys()]\n",
    "        custom_handles.append(plt.axvline(x=0, color='red', linestyle='--', label='Ideal Tolerance Factor'))\n",
    "        \n",
    "        final_handles = custom_handles + hue_handles\n",
    "        final_labels = [h.get_label() for h in custom_handles] + hue_labels\n",
    "        \n",
    "        plt.legend(handles=final_handles, labels=final_labels, title=\"Legend\")\n",
    "        plt.savefig(self.output_dir/'formation_energy_vs_tolerance.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def ensemble_predict(self, X):\n",
    "        weights = {'xgb': 0.3, 'rf': 0.2, 'gbr': 0.2, 'nn': 0.3}\n",
    "        preds = np.zeros(len(X))\n",
    "        for name, weight in weights.items():\n",
    "            if name == 'nn':\n",
    "                with torch.no_grad():\n",
    "                    pred = self.models[name]['model'](torch.FloatTensor(X)).numpy().flatten()\n",
    "            else:\n",
    "                pred = self.models[name].predict(X)\n",
    "            preds += weight * pred\n",
    "        return preds\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = PipelineConfig(val_size=0.15)\n",
    "    pipeline = PerovskitePipeline(config)\n",
    "    results, validation = pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d40b1-ae76-4693-b910-868548f9e95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
